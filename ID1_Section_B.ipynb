{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID1: Section B \u2014 Building an AI Model for Polymer Identification\n",
    "\n",
    "**Assigned Polymer:** PET (Polyethylene terephthalate)\n",
    "\n",
    "This notebook implements the data exploration, visualisation, and machine learning tasks required for Section B of the ID1 lab. NIR spectral data collected using the PlasTell spectrophotometer is processed, cleaned, and used to train k-Nearest Neighbours (kNN) classifiers for polymer identification.\n",
    "\n",
    "**Data sources:**\n",
    "- `matoha-data_3.csv` \u2014 Real PlasTell export collected during the lab session (334 spectra, 15 polymer types)\n",
    "- `data_source2.csv` \u2014 Lab-provided reference spectra to combine with own data in Task 2.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Imports and Configuration\n\nBefore any analysis, we need to load the Python **libraries** (pre-written code packages) that provide the tools we'll use throughout:\n\n| Library | What it does | Why we need it |\n|---|---|---|\n| `pandas` | Handles tabular data (like spreadsheets) | Loading CSV files, filtering columns, counting values |\n| `numpy` | Fast maths on arrays of numbers | Calculating means, standard deviations, manipulating spectral arrays |\n| `matplotlib` / `seaborn` | Plotting graphs and heatmaps | Visualising spectra, confusion matrices, PCA scatter plots |\n| `ast` | Safely converts text strings to Python objects | The PlasTell CSV stores spectra as text like `\"[54406, 54428, ...]\"` \u2014 we need to convert these to actual number lists |\n| `hashlib` | Generates unique \"fingerprints\" (hashes) for data | Used to detect duplicate spectra \u2014 two identical spectra will produce the same hash |\n| `sklearn` | Machine learning toolkit | Train/test splitting, scaling, kNN classifiers, evaluation metrics |\n\nWe also set a **random seed** and define **file paths** \u2014 both explained in the comments below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Data handling libraries \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import pandas as pd          # For loading/manipulating CSV data as tables (\"DataFrames\")\n",
    "import numpy as np           # For numerical operations on arrays (mean, std, etc.)\n",
    "\n",
    "# \u2500\u2500 Plotting libraries \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import matplotlib.pyplot as plt  # Core plotting library (line plots, scatter, bar charts)\n",
    "import seaborn as sns            # Higher-level plotting built on matplotlib (heatmaps, styled plots)\n",
    "\n",
    "# \u2500\u2500 Utility libraries \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import ast                   # ast.literal_eval() safely converts a string like \"[1, 2, 3]\" into an actual Python list\n",
    "import hashlib               # Generates SHA-256 hashes \u2014 a \"digital fingerprint\" to detect duplicate spectra\n",
    "import csv                   # (Not actually used, but available for CSV manipulation if needed)\n",
    "import os                    # For building file paths that work on any operating system\n",
    "\n",
    "# \u2500\u2500 Data structures \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from collections import Counter  # Quickly counts how many times each item appears in a list\n",
    "\n",
    "# \u2500\u2500 Machine learning (scikit-learn) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from sklearn.model_selection import train_test_split   # Splits data into training set and test set\n",
    "from sklearn.preprocessing import StandardScaler       # Centres and scales features to mean=0, std=1\n",
    "from sklearn.neighbors import KNeighborsClassifier     # The kNN classification algorithm\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # Evaluate how well the model performed\n",
    "\n",
    "# \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Many ML operations involve randomness (e.g., which spectra go into the\n",
    "# training vs test set). Setting a \"seed\" fixes the random number generator\n",
    "# so that every time you run this notebook, you get EXACTLY the same split,\n",
    "# the same results, and the same plots. The number 42 is arbitrary \u2014 any\n",
    "# integer would work \u2014 but using the same seed means your results are\n",
    "# reproducible by anyone who runs this code. This is essential for scientific\n",
    "# work: an examiner re-running your notebook should see identical output.\n",
    "np.random.seed(42)\n",
    "\n",
    "# \u2500\u2500 File paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# os.path.join() builds file paths using the correct separator for your OS\n",
    "# (/ on Mac/Linux, \\ on Windows). The '.' means \"current directory\" \u2014 i.e.,\n",
    "# the folder this notebook is saved in.\n",
    "DATA_DIR = os.path.join('.', 'data')       # Where the raw input CSV files live\n",
    "OUTPUT_DIR = os.path.join('.', 'output')   # Where we'll save processed/cleaned files\n",
    "\n",
    "# Input file paths \u2014 these point to the two data sources described in cell 0\n",
    "PLASTELL_CSV = os.path.join(DATA_DIR, 'matoha-data_3.csv')       # Your PlasTell spectrophotometer export\n",
    "DATA_SOURCE2_CSV = os.path.join(DATA_DIR, 'data_source2.csv')    # Lab-provided reference spectra\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 2: Exploring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.01: Counting Spectra & Polymers from your .CSV File\n\n**Objective:** Load the NIR spectral data from the PlasTell .CSV export, count the total number of spectra, and determine how many times each unique polymer appears in the dataset.\n\n**Why this matters:** Before doing any analysis or machine learning, you need to know:\n- **How much data you have** \u2014 too little data makes ML models unreliable.\n- **How the data is distributed across classes** \u2014 if one polymer has 100 spectra and another has 3, the model will be biased towards the larger class (this is called **class imbalance**)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_spectra(filepath):\n    \"\"\"Load a CSV file and count how many spectra (rows) it contains.\n    \n    Each row in the PlasTell export represents one spectrum measurement\n    taken from one polymer sample.\n    \"\"\"\n    df = pd.read_csv(filepath)   # pd.read_csv() reads a CSV file into a DataFrame (table)\n    total = len(df)              # len() counts the number of rows = number of spectra\n    print(f'Total number of spectra: {total}.')\n    return df                    # Return the DataFrame so we can use it in later tasks\n\n\ndef count_polymers(df):\n    \"\"\"Count how many spectra exist for each unique polymer type.\n    \n    The 'labelMaterialsString' column contains the polymer name (e.g., 'PET',\n    'HDPE') for each spectrum. value_counts() tallies how many times each\n    unique name appears \u2014 like doing a frequency count in a spreadsheet.\n    \"\"\"\n    counts = df['labelMaterialsString'].value_counts()\n    print(f'\\nNumber of spectra for each unique polymer:')\n    print(counts)\n    return counts\n\n\n# \u2500\u2500 Run Task 2.01 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Load the raw PlasTell data and count what we have\nraw_df = count_spectra(PLASTELL_CSV)\npolymer_counts = count_polymers(raw_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The dataset contains spectra for several polymer types. The distribution shows whether the dataset is **balanced** (roughly equal numbers of each polymer) or **imbalanced** (some polymers hugely over- or under-represented).\n\n**Why balance matters:** If PMMA has 100 spectra but ABS only has 5, a lazy model could predict \"PMMA\" for everything and still get ~95% accuracy \u2014 without actually learning to identify ABS. This is why we check the distribution early and address it later (see the Class Imbalance section)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.02: Filtering and Saving Specific Data\n\n**Objective:** Extract only the two columns we actually need \u2014 the polymer label and the spectrum data \u2014 and save them to a new, cleaner CSV file.\n\n**Why we do this:** The raw PlasTell export contains many columns of metadata (timestamps, device info, scan settings, etc.) that are irrelevant for ML. Keeping only `labelMaterialsString` (the polymer name) and `spectrum` (the intensity values) removes clutter and reduces file size. Think of it like highlighting only the relevant columns in a spreadsheet and copying them to a new sheet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def filter_and_save(df, output_path):\n    \"\"\"Keep only the polymer label and spectrum columns; discard everything else.\n    \n    .copy() creates an independent copy of the data so that changes to the\n    filtered version don't accidentally modify the original DataFrame.\n    \"\"\"\n    filtered = df[['labelMaterialsString', 'spectrum']].copy()  # Select just these 2 columns\n    filtered.to_csv(output_path, index=False)                   # Save to CSV (index=False avoids adding a row-number column)\n    print(f'Filtered data saved to: {output_path}')\n    print(f'\\nFirst 5 rows:')\n    print(filtered.head())    # .head() shows the first 5 rows \u2014 a quick sanity check\n    return filtered\n\n\n# \u2500\u2500 Run Task 2.02 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfiltered_path = os.path.join(OUTPUT_DIR, '2.02_filtered-data.csv')\nfiltered_df = filter_and_save(raw_df, filtered_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.03: Transforming Spectrophotometer Data\n\n**Objective:** Convert the raw data into a format suitable for analysis and ML.\n\n**The problem:** In the PlasTell CSV, each spectrum is stored as a **text string** that looks like a Python list: `\"[54406, 54428, 54691, ...]\"`. Python can't do maths on text \u2014 we need to convert these strings into actual arrays of numbers.\n\n**What this function does, step by step:**\n1. Uses `ast.literal_eval()` to safely convert each text string into a real Python list of integers.\n2. Calculates the **wavelength axis** \u2014 the PlasTell scans from 1550 nm to 1950 nm across 128 evenly-spaced points. `np.linspace(1550, 1950, 128)` generates these 128 wavelength values.\n3. **Transposes** the data so that each row = one wavelength and each column = one spectrum. This is the standard format for spectral databases (wavelengths on the y-axis, samples along the x-axis).\n4. Names each column by its polymer type (e.g., `HDPE`, `HDPE.01`, `HDPE.02`, ...) so we can identify which spectrum belongs to which polymer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_spectral_data(input_path, output_path):\n    \"\"\"Convert string-encoded spectra into a numeric wavelength x spectra table.\n    \n    Input format:  Each row has a polymer label and a spectrum stored as text.\n    Output format: Each row is a wavelength (1550-1950 nm), each column is one spectrum.\n    \"\"\"\n    df = pd.read_csv(input_path)\n    \n    # Step 1: Convert spectrum strings to actual Python lists of numbers\n    # e.g., the string \"[54406, 54428, ...]\" becomes the list [54406, 54428, ...]\n    df['spectrum'] = df['spectrum'].apply(ast.literal_eval)\n    \n    # Step 2: Calculate the wavelength axis\n    # The PlasTell measures 128 intensity values evenly spaced from 1550 nm to 1950 nm\n    num_points = len(df['spectrum'].iloc[0])           # How many points per spectrum (should be 128)\n    wavelengths = np.linspace(1550, 1950, num_points)  # Generate 128 evenly-spaced wavelengths\n    \n    # Step 3: Build unique column names for each spectrum\n    # First HDPE spectrum \u2192 \"HDPE\", second \u2192 \"HDPE.01\", third \u2192 \"HDPE.02\", etc.\n    labels = df['labelMaterialsString'].values\n    polymer_counter = {}   # Tracks how many spectra we've seen for each polymer\n    column_names = []\n    for label in labels:\n        if label not in polymer_counter:\n            polymer_counter[label] = 0\n        polymer_counter[label] += 1\n        count = polymer_counter[label]\n        if count == 1:\n            column_names.append(label)                      # First spectrum: just \"HDPE\"\n        else:\n            column_names.append(f'{label}.{count - 1:02d}')  # Subsequent: \"HDPE.01\", \"HDPE.02\", etc.\n    \n    # Step 4: Transpose so rows = wavelengths, columns = spectra\n    # .tolist() converts each spectrum list into rows of a 2D array\n    # .T transposes from (n_spectra, 128) to (128, n_spectra)\n    spectra_array = np.array(df['spectrum'].tolist()).T\n    result = pd.DataFrame(spectra_array, columns=column_names)\n    result.insert(0, 'Wavelength (nm)', wavelengths)  # Add wavelength as the first column\n    \n    result.to_csv(output_path, index=False)\n    print(f'Transformed data saved to: {output_path}')\n    print(f'Shape: {result.shape[0]} wavelengths x {result.shape[1] - 1} spectra')\n    print(f'\\nFirst 5 rows:')\n    print(result.head())\n    return result\n\n\n# \u2500\u2500 Run Task 2.03 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntransformed_path = os.path.join(OUTPUT_DIR, '2.03_transformed-data.csv')\ntransformed_df = transform_spectral_data(filtered_path, transformed_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in the required transposed format: each row corresponds to a wavelength value (1550\u20131950 nm) and each column holds a single spectrum labelled by polymer type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.04: Combining Datasets\n\n**Objective:** Merge your own PlasTell spectra with the lab-provided reference dataset (`data_source2.csv`) into one large combined database.\n\n**Why we do this:** Your PlasTell data contains 334 spectra across 15 polymer types (6\u201337 per polymer). The lab-provided dataset adds a further 335 spectra, roughly doubling the training data available for ML. More data = better generalisation = more reliable predictions.\n\n**How it works:** Both datasets have the same structure \u2014 128 wavelength points from 1550-1950 nm \u2014 so we can simply stick them side-by-side (column-wise concatenation). We drop the duplicate wavelength column from the second dataset to avoid having it twice."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def combine_datasets(path1, path2, output_path):\n    \"\"\"Combine two spectral datasets side-by-side (column-wise).\n    \n    Both datasets must have the same number of rows (128 wavelengths).\n    The wavelength column from dataset 2 is dropped to avoid duplication.\n    \"\"\"\n    df1 = pd.read_csv(path1)   # Our transformed PlasTell data\n    df2 = pd.read_csv(path2)   # The lab-provided reference spectra\n    \n    # Drop the wavelength column from dataset 2 \u2014 we already have it from dataset 1\n    df2_spectra = df2.iloc[:, 1:]   # iloc[:, 1:] means \"all rows, columns from index 1 onwards\" (skipping column 0)\n    \n    # pd.concat with axis=1 joins tables side-by-side (adding columns)\n    # axis=0 would stack them vertically (adding rows)\n    combined = pd.concat([df1, df2_spectra], axis=1)\n    combined.to_csv(output_path, index=False)\n    \n    print(f'Combined data saved to: {output_path}')\n    print(f'Dataset 1: {df1.shape[1] - 1} spectra')       # -1 to exclude the wavelength column\n    print(f'Dataset 2: {df2_spectra.shape[1]} spectra')\n    print(f'Combined: {combined.shape[1] - 1} total spectra')\n    return combined\n\n\n# \u2500\u2500 Run Task 2.04 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ncombined_path = os.path.join(OUTPUT_DIR, '2.04_combined-data.csv')\ncombined_df = combine_datasets(transformed_path, DATA_SOURCE2_CSV, combined_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.05: Counting Columns by Polymer\n\n**Objective:** Now that we've combined both datasets, count how many spectra we have for each polymer type.\n\n**How column names encode the polymer:** Each column is named like `POLYMER` or `POLYMER.XX` (e.g., `PMMA`, `PMMA.01`, `PMMA.02`). By splitting on the `.` character and taking the first part, we extract the polymer name. Then `Counter` tallies up the totals \u2014 this is our \"class distribution\" for ML."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_columns_by_polymer(filepath):\n    \"\"\"Count how many spectra columns exist for each polymer type.\n    \n    Column names like 'PMMA.05' are split on '.' to extract just 'PMMA'.\n    Counter then tallies how many columns share each polymer prefix.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    headers = df.columns[1:]  # Skip the first column (wavelength) \u2014 everything else is a spectrum\n    \n    # Extract polymer name from each column header\n    # 'PMMA.05'.split('.')[0] \u2192 'PMMA'\n    # 'PET'.split('.')[0] \u2192 'PET' (still works even without a dot)\n    prefixes = [h.split('.')[0] for h in headers]\n    counts = Counter(prefixes)  # Counter({'PMMA': 118, 'PVC': 73, ...})\n    \n    print(f'Total number of spectra columns: {len(headers)}')\n    print(f'\\nSpectra count per polymer type:')\n    for polymer, count in sorted(counts.items()):\n        print(f'  {polymer}: {count}')\n    return counts\n\n\n# \u2500\u2500 Run Task 2.05 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npolymer_column_counts = count_columns_by_polymer(combined_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced polymer representation can bias the kNN model towards over-represented classes. If significant imbalances exist, strategies such as undersampling, oversampling, or stratified splitting should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.06: Looking for Duplicates\n\n**Objective:** Detect and remove spectra that are exact copies of each other.\n\n**Why duplicates are a problem:** When we combined datasets (Task 2.04), some spectra from our PlasTell data may already exist in the lab-provided dataset \u2014 they'd be counted twice. Duplicates artificially inflate the dataset and bias the ML model by over-representing those particular measurements.\n\n**How we detect them \u2014 SHA-256 hashing:**\n- A **hash function** takes any data and produces a fixed-length \"fingerprint\" string (e.g., `a3f5b2c1...`).\n- If two spectra contain **exactly the same** 128 intensity values, they produce **the same hash**.\n- If even a single value differs, the hashes will be completely different.\n- This lets us efficiently compare hundreds of spectra without checking every value pair-by-pair.\n\nWe keep the first copy and remove all subsequent duplicates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_hash(spectrum_values):\n    \"\"\"Generate a unique SHA-256 fingerprint for a single spectrum.\n    \n    Converts the 128 intensity values to a string, then feeds it through\n    the SHA-256 algorithm to produce a 64-character hex digest. Two spectra\n    with identical values will always produce the same hash.\n    \"\"\"\n    data_string = str(spectrum_values.tolist())                     # Convert array to string: \"[54406, 54428, ...]\"\n    return hashlib.sha256(data_string.encode('utf-8')).hexdigest()  # Hash it \u2192 64-char hex string\n\n\ndef find_and_remove_duplicates(filepath, output_path):\n    \"\"\"Find spectra that are exact copies and remove the duplicates.\n    \n    Strategy: Hash each spectrum column. If two columns share the same hash,\n    they contain identical data. Keep the first occurrence, drop the rest.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    spectra_columns = df.columns[1:]  # Everything except the wavelength column\n    \n    # Build a dictionary: hash \u2192 list of column names that share that hash\n    hash_dict = {}\n    for col in spectra_columns:\n        h = compute_hash(df[col])\n        if h not in hash_dict:\n            hash_dict[h] = []\n        hash_dict[h].append(col)\n    \n    # Any hash with more than one column = those columns are duplicates\n    duplicates = {h: cols for h, cols in hash_dict.items() if len(cols) > 1}\n    \n    if duplicates:\n        print('Duplicate spectra found:')\n        columns_to_drop = []\n        for h, cols in duplicates.items():\n            print(f'  Columns with identical spectra: {cols}')\n            columns_to_drop.extend(cols[1:])   # Keep the first, mark the rest for removal\n        \n        df_cleaned = df.drop(columns=columns_to_drop)  # Remove duplicate columns\n        print(f'\\nRemoved {len(columns_to_drop)} duplicate column(s).')\n    else:\n        print('No duplicate spectra found.')\n        df_cleaned = df\n    \n    df_cleaned.to_csv(output_path, index=False)\n    print(f'Duplicate-checked data saved to: {output_path}')\n    print(f'Remaining spectra: {df_cleaned.shape[1] - 1}')\n    return df_cleaned\n\n\n# \u2500\u2500 Run Task 2.06 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndup_checked_path = os.path.join(OUTPUT_DIR, '2.06_duplicate-checked-data.csv')\ndup_checked_df = find_and_remove_duplicates(combined_path, dup_checked_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.07: Checking for Invalid Values in Transformed Data\n\n**Objective:** Scan every data point in the dataset for **missing values** (NaN) and **non-numeric entries** (e.g., the word `\"none\"` appearing where a number should be), then repair them using an appropriate method.\n\n**Why this matters:** ML algorithms require clean numerical input. If even one cell contains text like `\"none\"` instead of a number, the model will crash or produce nonsensical results. The lab-provided `data_source2.csv` is known to contain some `\"none\"` entries \u2014 these must first be converted to `NaN` (a numerical placeholder meaning \"no value\"), then replaced with a reasonable estimate.\n\n**Approaches for handling invalid values in spectral data:**\n\n| Method | How it works | Strengths | Weaknesses |\n|---|---|---|---|\n| **Linear interpolation** (along wavelength axis) | Uses the valid data points immediately before and after the gap *within the same spectrum* to estimate the missing value | Preserves the individual spectrum's shape; physically meaningful because adjacent NIR wavelengths are highly correlated | Cannot handle missing values at the very start or end of a spectrum (no neighbour on one side) |\n| **Spline interpolation** | Fits a smooth curve through multiple surrounding points within the same spectrum | Smoother than linear interpolation; better for consecutive missing points | Computationally heavier; can overshoot if the gap is large |\n| **Column median imputation** (across spectra) | Replaces the missing value with the median of all other spectra at that same wavelength | Simple to implement; robust to outliers | Physically questionable \u2014 different polymers have very different intensities at any given wavelength, so the median across polymers is not a meaningful estimate for any single spectrum |\n| **Column mean imputation** (across spectra) | Same as median but uses the mean | Simple to implement | Same problem as median, and additionally sensitive to outliers \u2014 one extreme value skews the replacement |\n| **Discard the spectrum** | Remove the entire spectrum if it contains invalid values | Guarantees clean data with no estimated values | Wasteful if only 1\u20132 points out of 128 are affected; reduces dataset size |\n\n**Why we chose linear interpolation:** In NIR spectroscopy, the signal varies smoothly and continuously across adjacent wavelengths \u2014 there are no sharp discontinuities in a real spectrum. This means the best estimate for a missing point comes from its immediate spectral neighbours *within the same measurement*, not from other samples measured under different conditions. Linear interpolation exploits this physical property directly. For any edge cases where the missing value falls at the very first or last wavelength (where one-sided interpolation is impossible), we fall back to forward- or back-filling from the nearest valid point \u2014 a standard approach in signal processing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_and_fix_invalid_values(filepath, output_path):\n    \"\"\"Scan for missing/non-numeric values and repair them using linear interpolation.\n    \n    Two types of problems are checked:\n    1. NaN (Not a Number) \u2014 empty cells or values that pandas couldn't parse\n    2. Non-numeric strings \u2014 e.g., \"none\" appearing where a number should be\n    \n    Repair strategy (linear interpolation along wavelength axis):\n    - Each spectrum column is treated independently.\n    - Non-numeric strings are first converted to NaN.\n    - pandas .interpolate(method='linear') estimates each NaN from the valid\n      data points immediately above and below it in the same column (i.e., the\n      adjacent wavelengths in the same spectrum).\n    - Edge cases (NaN at row 0 or row 127) are handled by forward/back-fill.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    issues_found = False\n    columns_with_issues = []\n    \n    for col in df.columns[1:]:  # Skip the wavelength column \u2014 only check spectrum columns\n        col_had_issue = False\n        \n        # Check 1: Are there any NaN (missing) values?\n        nan_mask = df[col].isna()          # Creates True/False for each cell: True = missing\n        if nan_mask.any():                  # .any() returns True if at least one cell is NaN\n            issues_found = True\n            col_had_issue = True\n            nan_rows = df.index[nan_mask].tolist()\n            print(f'  Missing value (NaN) in column \"{col}\" at row(s): {[r + 2 for r in nan_rows]}')\n        \n        # Check 2: Are there any values that look like numbers but are actually text?\n        for idx, value in df[col].items():\n            if pd.isna(value):\n                continue                    # Already flagged above, skip it\n            try:\n                float(value)                # Try converting to a number\n            except (ValueError, TypeError):  # If this fails, it's not a valid number\n                issues_found = True\n                col_had_issue = True\n                print(f'  Non-numeric value \"{value}\" in column \"{col}\" at row {idx + 2}')\n                df.at[idx, col] = np.nan    # Mark it as NaN so we can interpolate it below\n        \n        if col_had_issue:\n            columns_with_issues.append(col)\n    \n    if not issues_found:\n        print('No invalid values found.')\n    else:\n        # Repair invalid values using linear interpolation along the wavelength axis\n        print('\\nRepairing invalid values using linear interpolation...')\n        for col in columns_with_issues:\n            df[col] = pd.to_numeric(df[col], errors='coerce')  # Force everything to numbers (non-numeric \u2192 NaN)\n            \n            n_missing = df[col].isna().sum()\n            \n            # Linear interpolation: estimates each NaN from its immediate neighbours\n            # in the same column (i.e., adjacent wavelengths in the same spectrum).\n            # This is preferred over cross-spectrum imputation because adjacent\n            # wavelengths in NIR are highly correlated within a single measurement.\n            df[col] = df[col].interpolate(method='linear')\n            \n            # Handle edge cases: if NaN was at the very first or last wavelength,\n            # interpolation can't look in both directions. Forward-fill (ffill) and\n            # back-fill (bfill) propagate the nearest valid value to cover these.\n            df[col] = df[col].ffill().bfill()\n            \n            print(f'  Column \"{col}\": repaired {n_missing} value(s) via linear interpolation')\n    \n    df.to_csv(output_path, index=False)\n    print(f'\\nChecked data saved to: {output_path}')\n    return df\n\n\n# \u2500\u2500 Run Task 2.07 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nchecked_path = os.path.join(OUTPUT_DIR, '2.07_checked-data.csv')\nchecked_df = check_and_fix_invalid_values(dup_checked_path, checked_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Summary of repaired values:**\n\nTwo invalid entries (`\"none\"`) were found in the `HDPE.5` spectrum column. The table below shows the original invalid values, what linear interpolation produced, and what the old column-median approach would have given for comparison:\n\n| Wavelength (nm) | Original value | Neighbour before | Neighbour after | Linear interpolation | Column median (not used) |\n|---|---|---|---|---|---|\n| 1562.6 | `\"none\"` | 56 179 (at 1559.4 nm) | 57 077 (at 1565.7 nm) | **56 628** | 46 298 |\n| 1578.3 | `\"none\"` | 58 861 (at 1575.2 nm) | 60 164 (at 1581.5 nm) | **59 513** | 46 298 |\n\nThe difference between the two methods is substantial: the column median (46 298) is ~10 000\u201313 000 counts lower than the interpolated values because it averages across all polymers at that wavelength \u2014 including polymers with much lower intensities in that spectral region. Linear interpolation produces values that sit naturally between their immediate spectral neighbours, preserving the smooth upward trend of the HDPE spectrum in the 1550\u20131580 nm region. Using the column median here would have introduced an artificial dip into the spectrum, potentially distorting downstream classification."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 3: Visualising Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 3.01: Visualisation of Central Tendency & Dispersion\n\n**Objective:** For each polymer type, calculate the **mean spectrum** (average shape) and **standard deviation** (how much individual spectra vary from the average), then plot both.\n\n**Why this matters:**\n- The **mean spectrum** shows the \"typical\" NIR signature for each polymer \u2014 this is what the ML model is essentially trying to learn for each class.\n- The **standard deviation** (shown as a shaded band around the mean) shows how much **variation** exists within each polymer class. Large variation = harder for the model to learn a clear pattern.\n- If two polymers' mean spectra look very similar, the model will struggle to tell them apart.\n\n**Why we use the mean (not the median) here:** For continuous spectral intensity data that is roughly symmetrically distributed at each wavelength, the mean is a good summary. (We used the median earlier for *imputation* because it's more robust to extreme outliers \u2014 different use case.)\n\n**Why the x-axis is reversed:** In spectroscopy, it's convention to plot with higher wavenumbers (shorter wavelengths) on the left. Since we're plotting wavelength in nm, reversing the axis follows this convention."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_prefix(column_name):\n    \"\"\"Extract the polymer name from a column header.\n    \n    Examples:\n        'PET'    \u2192 'PET'     (no dot, returns the whole name)\n        'PET.03' \u2192 'PET'     (splits on '.', returns the first part)\n        'PMMA.105' \u2192 'PMMA'\n    \n    This is used throughout the notebook whenever we need to group\n    spectra by their polymer type.\n    \"\"\"\n    return column_name.split('.')[0]\n\n\ndef plot_average_spectra(filepath, output_csv_path):\n    \"\"\"Calculate and plot the mean spectrum +/- standard deviation for each polymer.\n    \n    For each polymer group:\n    1. Collect all spectra belonging to that polymer\n    2. Calculate the mean intensity at each wavelength (the average shape)\n    3. Calculate the standard deviation at each wavelength (the spread)\n    4. Plot the mean as a line and the +/- 1 std dev as a shaded band\n    \"\"\"\n    df = pd.read_csv(filepath)\n    wavelengths = df.iloc[:, 0].values   # First column = wavelength values (1550-1950 nm)\n    spectra_columns = df.columns[1:]     # All other columns = individual spectra\n    \n    # Group columns by polymer prefix (e.g., all PMMA columns together)\n    groups = {}\n    for col in spectra_columns:\n        prefix = extract_prefix(col)\n        if prefix not in groups:\n            groups[prefix] = []\n        groups[prefix].append(col)\n    \n    # Calculate mean and std for each polymer group, and plot\n    results = {'Wavelength (nm)': wavelengths}\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for polymer in sorted(groups.keys()):\n        cols = groups[polymer]\n        # Convert to numeric (in case any strings remain), shape: (128 wavelengths, n_spectra)\n        data = df[cols].apply(pd.to_numeric, errors='coerce').values\n        mean_spectrum = np.nanmean(data, axis=1)  # Average across spectra at each wavelength\n        std_spectrum = np.nanstd(data, axis=1)    # Std dev across spectra at each wavelength\n        \n        results[f'{polymer}_mean'] = mean_spectrum\n        results[f'{polymer}_std'] = std_spectrum\n        \n        # Plot the mean line\n        ax.plot(wavelengths, mean_spectrum, label=polymer)\n        # Plot the shaded band showing +/- 1 standard deviation\n        ax.fill_between(wavelengths,\n                        mean_spectrum - std_spectrum,    # Lower bound\n                        mean_spectrum + std_spectrum,    # Upper bound\n                        alpha=0.2)                       # alpha=0.2 makes it semi-transparent\n    \n    ax.set_xlabel('Wavelength (nm)')\n    ax.set_ylabel('Intensity (a.u.)')         # a.u. = arbitrary units (PlasTell raw counts)\n    ax.set_title('Average Spectra with Standard Deviation')\n    ax.legend(loc='best', fontsize=8)\n    ax.invert_xaxis()                          # Spectroscopy convention: high wavenumber on left\n    plt.tight_layout()\n    plt.show()\n    \n    # Save the averaged data to CSV for reference\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(output_csv_path, index=False)\n    print(f'Averaged data saved to: {output_csv_path}')\n    return results_df\n\n\n# \u2500\u2500 Run Task 3.01 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\naveraged_path = os.path.join(OUTPUT_DIR, '3.01_averaged-data.csv')\naveraged_df = plot_average_spectra(checked_path, averaged_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that different polymers have distinct spectral signatures, particularly in the 1650\u20131850 nm region. Large error bars may indicate measurement variability or outlier spectra within that polymer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.02: Investigating Spectral Variation \u2014 The Colour Problem\n",
    "\n",
    "**Objective:** Generate per-polymer heatmaps of min-max normalised spectra, identify which spectra belong to dark-coloured samples, and separate them from light-coloured samples to prevent contamination of the training data.\n",
    "\n",
    "#### Why dark plastics produce different spectra\n",
    "\n",
    "The PlasTell Desktop uses NIR spectroscopy (1550\u20131950 nm), which relies on molecular absorption features \u2014 specific wavelengths where C-H, O-H, N-H, and other bonds absorb light. In dark or black plastics, **carbon black** is the most common pigment. Carbon black absorbs broadly across the entire NIR spectrum, which:\n",
    "\n",
    "1. **Suppresses diagnostic absorption peaks** \u2014 the carbon black absorption dominates, masking the polymer-specific features that the classifier needs.\n",
    "2. **Reduces overall signal intensity** \u2014 less light reaches the detector, lowering the signal-to-noise ratio.\n",
    "3. **Causes the PlasTell to report `measured = unknown`** \u2014 the device's built-in AI cannot identify the polymer because the spectral features it was trained on are absent.\n",
    "\n",
    "This is not a random measurement error \u2014 it is a **systematic physical effect** tied to the sample's colour. Treating these spectra as \"outliers\" is misleading because they are not measurement failures; they are valid measurements of a fundamentally different optical situation (polymer + strong broadband absorber).\n",
    "\n",
    "#### Colour metadata in our dataset\n",
    "\n",
    "Our PlasTell data includes a `colour` column recorded during measurement. The following dark-coloured samples were identified:\n",
    "\n",
    "| Polymer | Colour | Count | Comment / sample description | Device result |\n",
    "|---|---|---|---|---|\n",
    "| HDPE | black | 6 | `HDPE_black100mLbottle` | `unknown` |\n",
    "| PET | black | 5 | `PET_bottle-b` (black bottles) | `unknown` |\n",
    "| PET | brown | 5 | `PET_ambertub` (amber/brown tubs) | OK |\n",
    "| PP | black | 7 | `PP_square` (black squares) | `unknown` |\n",
    "| PS | grey | 5 | `HIPS_grey` (high-impact PS, grey) | `unknown` |\n",
    "| PLA | grey | 6 | `PLA_printed_grey` (3D-printed grey) | `unknown` |\n",
    "| PVC | black | 5 | `PVC_blacktube` (black tubes) | `unknown` |\n",
    "\n",
    "**Total: 39 dark spectra** out of 334 PlasTell spectra (11.7%). All black and grey samples measured as `unknown`, confirming the NIR-absorption effect. The brown PET (amber tubs) were still identified correctly \u2014 brown allows more NIR transmission than black.\n",
    "\n",
    "The lab-provided reference data (`data_source2`, 335 spectra) does not include colour metadata, so those spectra remain in the main group.\n",
    "\n",
    "#### Approach: colour-based separation instead of statistical outlier detection\n",
    "\n",
    "Rather than applying a statistical method (Pearson correlation, PCA, Mahalanobis distance) that blindly flags spectra without understanding *why* they differ, we take a physically motivated approach:\n",
    "\n",
    "1. **Map colour metadata** from the original PlasTell export onto the transformed column names.\n",
    "2. **Define \"dark\" colours** as `black`, `grey`, and optionally `brown` \u2014 colours known to interfere with NIR spectroscopy.\n",
    "3. **Plot separate heatmaps** for dark and light spectra within each polymer group, so the separation can be visually verified.\n",
    "4. **Remove dark spectra** from the main training set, as including them would confuse the classifier with featureless or distorted spectra.\n",
    "\n",
    "This approach is more transparent and physically justified than automated statistical detection. It also allows the user to visually inspect each separation and confirm that the flagged spectra genuinely correspond to dark samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_colour_map(original_data_path):\n",
    "    \"\"\"Build a mapping from transformed column names to colour metadata.\n",
    "    \n",
    "    The PlasTell export includes a 'colour' column for each measurement.\n",
    "    This function recreates the column naming used during transformation\n",
    "    (Task 2.03) to map each spectrum column back to its recorded colour.\n",
    "    \n",
    "    Args:\n",
    "        original_data_path: path to the raw PlasTell CSV (matoha-data_3.csv).\n",
    "    \n",
    "    Returns:\n",
    "        colour_map: dict mapping column name (e.g., 'HDPE.10') to colour string.\n",
    "        comment_map: dict mapping column name to sample description comment.\n",
    "    \"\"\"\n",
    "    df_orig = pd.read_csv(original_data_path)\n",
    "    polymer_counter = {}\n",
    "    colour_map = {}\n",
    "    comment_map = {}\n",
    "    \n",
    "    for _, row in df_orig.iterrows():\n",
    "        polymer = row['labelMaterialsString']\n",
    "        colour = row['colour']\n",
    "        comment = row.get('comment', '')\n",
    "        \n",
    "        if polymer not in polymer_counter:\n",
    "            polymer_counter[polymer] = 0\n",
    "        \n",
    "        idx = polymer_counter[polymer]\n",
    "        col_name = f\"{polymer}.{idx:02d}\" if idx > 0 else polymer\n",
    "        # Handle the case where first spectrum has no suffix\n",
    "        # Check both formats since the combined data may use either\n",
    "        polymer_counter[polymer] += 1\n",
    "        \n",
    "        colour_map[col_name] = colour\n",
    "        comment_map[col_name] = comment if pd.notna(comment) else ''\n",
    "    \n",
    "    return colour_map, comment_map\n",
    "\n",
    "\n",
    "def plot_heatmaps_by_colour(filepath, output_path, original_data_path,\n",
    "                             dark_colours={'black'}, manual_dark=None,\n",
    "                             measurement_outliers=None):\n",
    "    \"\"\"Create per-polymer heatmaps with dark/light colour separation.\n",
    "    \n",
    "    For each polymer group that contains dark-coloured spectra:\n",
    "    - Two heatmaps are plotted side-by-side: light (left) and dark (right).\n",
    "    - Dark spectra labels are shown in red with their recorded colour.\n",
    "    \n",
    "    For polymer groups with no dark spectra, a single heatmap is shown.\n",
    "    Dark spectra are removed from the output dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the checked spectral data CSV.\n",
    "        output_path: path to save the cleaned (light-only) output CSV.\n",
    "        original_data_path: path to raw PlasTell CSV for colour metadata.\n",
    "        dark_colours: set of colour strings to classify as \"dark\".\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    wavelengths = df.iloc[:, 0].values\n",
    "    spectra_columns = df.columns[1:]\n",
    "    \n",
    "    # Build colour map from original data\n",
    "    colour_map, comment_map = build_colour_map(original_data_path)\n",
    "    \n",
    "    # Fix: the first spectrum per polymer may be named without .00\n",
    "    # e.g., 'PP' instead of 'PP.00'. Remap if needed.\n",
    "    fixed_colour_map = {}\n",
    "    fixed_comment_map = {}\n",
    "    for col in spectra_columns:\n",
    "        if col in colour_map:\n",
    "            fixed_colour_map[col] = colour_map[col]\n",
    "            fixed_comment_map[col] = comment_map[col]\n",
    "        else:\n",
    "            # Try .00 suffix format\n",
    "            alt = f\"{col}.00\"\n",
    "            if alt in colour_map:\n",
    "                fixed_colour_map[col] = colour_map[alt]\n",
    "                fixed_comment_map[col] = comment_map[alt]\n",
    "    \n",
    "    colour_map = fixed_colour_map\n",
    "    comment_map = fixed_comment_map\n",
    "    \n",
    "    # Apply manual dark overrides (visually identified, no metadata)\n",
    "    if manual_dark:\n",
    "        for col in manual_dark:\n",
    "            if col in list(spectra_columns):\n",
    "                colour_map[col] = 'dark (manual)'\n",
    "                if col not in comment_map or not comment_map[col]:\n",
    "                    comment_map[col] = 'visually identified as dark'\n",
    "        print(f'Applied {len(manual_dark)} manual dark override(s): {manual_dark}\\n')\n",
    "    \n",
    "    # Apply measurement outlier removals\n",
    "    outlier_removals = []\n",
    "    if measurement_outliers:\n",
    "        for col in measurement_outliers:\n",
    "            if col in list(spectra_columns):\n",
    "                outlier_removals.append(col)\n",
    "        print(f'Measurement outliers to remove: {outlier_removals}\\n')\n",
    "    \n",
    "    # Group by polymer prefix\n",
    "    groups = {}\n",
    "    for col in spectra_columns:\n",
    "        prefix = extract_prefix(col)\n",
    "        if prefix not in groups:\n",
    "            groups[prefix] = []\n",
    "        groups[prefix].append(col)\n",
    "    \n",
    "    dark_columns = []\n",
    "    \n",
    "    for polymer in sorted(groups.keys()):\n",
    "        cols = groups[polymer]\n",
    "        data = df[cols].apply(pd.to_numeric, errors='coerce').values.T\n",
    "        \n",
    "        # Separate dark and light spectra\n",
    "        dark_cols = [c for c in cols if colour_map.get(c, '').lower() in dark_colours or colour_map.get(c, '') == 'dark (manual)']\n",
    "        outlier_cols = [c for c in cols if c in (measurement_outliers or [])]\n",
    "        dark_cols = [c for c in dark_cols if c not in outlier_cols]\n",
    "        light_cols = [c for c in cols if c not in dark_cols and c not in outlier_cols]\n",
    "        \n",
    "        dark_columns.extend(dark_cols)\n",
    "        dark_columns.extend(outlier_cols)\n",
    "        \n",
    "        # Print measurement outliers separately\n",
    "        if outlier_cols:\n",
    "            print(f'  {polymer}: {len(outlier_cols)} measurement outlier(s) removed: {outlier_cols}')\n",
    "        \n",
    "        if dark_cols:\n",
    "            # \u2500\u2500 Stacked heatmaps: light on top, dark below \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            wl_labels = [f'{w:.0f}' for w in wavelengths]\n",
    "            step = max(1, len(wl_labels) // 10)\n",
    "            \n",
    "            # Top: light spectra\n",
    "            light_data = df[light_cols].apply(pd.to_numeric, errors='coerce').values.T\n",
    "            light_norm = np.zeros_like(light_data, dtype=float)\n",
    "            for i in range(light_data.shape[0]):\n",
    "                row = light_data[i]\n",
    "                row_min, row_max = np.nanmin(row), np.nanmax(row)\n",
    "                if row_max - row_min > 0:\n",
    "                    light_norm[i] = (row - row_min) / (row_max - row_min)\n",
    "            \n",
    "            # Build labels with colour and comment for our PlasTell spectra\n",
    "            light_labels = []\n",
    "            for c in light_cols:\n",
    "                colour = colour_map.get(c, '')\n",
    "                comment = comment_map.get(c, '')\n",
    "                parts = [c]\n",
    "                if colour:\n",
    "                    parts.append(f'[{colour}]')\n",
    "                if comment:\n",
    "                    parts.append(f'\u2014 {comment}')\n",
    "                light_labels.append(' '.join(parts))\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, max(3, len(light_cols) * 0.3)))\n",
    "            light_df = pd.DataFrame(light_norm, index=light_labels, columns=wl_labels)\n",
    "            sns.heatmap(light_df, cmap='viridis', ax=ax, xticklabels=step, yticklabels=True)\n",
    "            ax.set_title(f'{polymer} \u2014 Light spectra ({len(light_cols)})')\n",
    "            ax.set_xlabel('Wavelength (nm)')\n",
    "            ax.set_ylabel('Spectrum')\n",
    "            ax.invert_xaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Bottom: dark spectra\n",
    "            dark_data = df[dark_cols].apply(pd.to_numeric, errors='coerce').values.T\n",
    "            dark_norm = np.zeros_like(dark_data, dtype=float)\n",
    "            for i in range(dark_data.shape[0]):\n",
    "                row = dark_data[i]\n",
    "                row_min, row_max = np.nanmin(row), np.nanmax(row)\n",
    "                if row_max - row_min > 0:\n",
    "                    dark_norm[i] = (row - row_min) / (row_max - row_min)\n",
    "            \n",
    "            dark_labels = []\n",
    "            for c in dark_cols:\n",
    "                colour = colour_map.get(c, '?')\n",
    "                comment = comment_map.get(c, '')\n",
    "                parts = [c]\n",
    "                if colour:\n",
    "                    parts.append(f'[{colour}]')\n",
    "                if comment:\n",
    "                    parts.append(f'\u2014 {comment}')\n",
    "                dark_labels.append(' '.join(parts))\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, max(2, len(dark_cols) * 0.4)))\n",
    "            dark_df = pd.DataFrame(dark_norm, index=dark_labels, columns=wl_labels)\n",
    "            sns.heatmap(dark_df, cmap='magma', ax=ax, xticklabels=step, yticklabels=True)\n",
    "            ax.set_title(f'{polymer} \u2014 Dark spectra ({len(dark_cols)}) \u2014 SEPARATED')\n",
    "            ax.set_xlabel('Wavelength (nm)')\n",
    "            ax.set_ylabel('Spectrum')\n",
    "            ax.invert_xaxis()\n",
    "            \n",
    "            for tick_label in ax.get_yticklabels():\n",
    "                tick_label.set_color('red')\n",
    "                tick_label.set_fontweight('bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'  {polymer}: {len(dark_cols)} dark spectra separated:')\n",
    "            for c in dark_cols:\n",
    "                colour = colour_map.get(c, '?')\n",
    "                comment = comment_map.get(c, '')\n",
    "                print(f'    - {c} (colour: {colour}, sample: {comment})')\n",
    "            print(f'  {polymer}: {len(light_cols)} light spectra retained.\\n')\n",
    "        \n",
    "        else:\n",
    "            # \u2500\u2500 Single heatmap: all light \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            normalised = np.zeros_like(data, dtype=float)\n",
    "            for i in range(data.shape[0]):\n",
    "                row = data[i]\n",
    "                row_min, row_max = np.nanmin(row), np.nanmax(row)\n",
    "                if row_max - row_min > 0:\n",
    "                    normalised[i] = (row - row_min) / (row_max - row_min)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, max(3, len(cols) * 0.3)))\n",
    "            wl_labels = [f'{w:.0f}' for w in wavelengths]\n",
    "            step = max(1, len(wl_labels) // 10)\n",
    "            \n",
    "            single_labels = []\n",
    "            for c in cols:\n",
    "                colour = colour_map.get(c, '')\n",
    "                comment = comment_map.get(c, '')\n",
    "                parts = [c]\n",
    "                if colour:\n",
    "                    parts.append(f'[{colour}]')\n",
    "                if comment:\n",
    "                    parts.append(f'\u2014 {comment}')\n",
    "                single_labels.append(' '.join(parts))\n",
    "            heatmap_df = pd.DataFrame(normalised, index=single_labels, columns=wl_labels)\n",
    "            sns.heatmap(heatmap_df, cmap='viridis', ax=ax, xticklabels=step, yticklabels=True)\n",
    "            ax.set_title(f'Heatmap of Normalised Spectra \u2014 {polymer} ({len(cols)} spectra, no dark samples)')\n",
    "            ax.set_xlabel('Wavelength (nm)')\n",
    "            ax.set_ylabel('Spectrum')\n",
    "            ax.invert_xaxis()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'  {polymer}: {len(cols)} spectra (no dark samples detected).\\n')\n",
    "    \n",
    "    # \u2500\u2500 Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'COLOUR-BASED SEPARATION SUMMARY')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'Dark colours flagged: {sorted(dark_colours)}')\n",
    "    print(f'Total dark spectra separated: {len(dark_columns)}')\n",
    "    \n",
    "    if dark_columns:\n",
    "        dark_by_polymer = {}\n",
    "        for col in dark_columns:\n",
    "            prefix = extract_prefix(col)\n",
    "            if prefix not in dark_by_polymer:\n",
    "                dark_by_polymer[prefix] = []\n",
    "            dark_by_polymer[prefix].append(col)\n",
    "        \n",
    "        for polymer in sorted(dark_by_polymer.keys()):\n",
    "            cols_list = dark_by_polymer[polymer]\n",
    "            colours = [colour_map.get(c, '?') for c in cols_list]\n",
    "            colour_summary = ', '.join(sorted(set(colours)))\n",
    "            print(f'  {polymer}: {len(cols_list)} ({colour_summary}) \u2014 {cols_list}')\n",
    "    \n",
    "    print(f'{\"=\"*60}\\n')\n",
    "    \n",
    "    # \u2500\u2500 Remove dark spectra and save \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    if dark_columns:\n",
    "        df_cleaned = df.drop(columns=dark_columns)\n",
    "        print(f'Removed {len(dark_columns)} dark spectra.')\n",
    "    else:\n",
    "        df_cleaned = df\n",
    "        print('No dark spectra found.')\n",
    "    \n",
    "    df_cleaned.to_csv(output_path, index=False)\n",
    "    print(f'Cleaned data saved to: {output_path}')\n",
    "    print(f'Remaining spectra: {df_cleaned.shape[1] - 1}')\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# \u2500\u2500 Manual dark overrides \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Spectra from data_source2 with no colour metadata, identified as dark\n",
    "# by visual inspection of the heatmaps.\n",
    "MANUAL_DARK_OVERRIDES = [\n",
    "    'HDPE.15.1', 'HDPE.16.1', 'HDPE.17.1', 'HDPE.18.1', 'HDPE.19.1',\n",
    "    'PVC.1', 'PVC.2', 'PVC.3', 'PVC.4', 'PVC.70',\n",
    "    'PVC.50', 'PVC.51', 'PVC.52', 'PVC.53', 'PVC.54',\n",
    "    'PVC.55', 'PVC.56', 'PVC.57', 'PVC.58', 'PVC.59',\n",
    "    'PP.20.1', 'PP.21.1', 'PP.22.1', 'PP.23.1', 'PP.24.1',\n",
    "    'PET.11.1', 'PET.12.1', 'PET.13.1',\n",
    "    'PMMA.85', 'PMMA.86', 'PMMA.87', 'PMMA.88', 'PMMA.89',\n",
    "    'PMMA.90', 'PMMA.91', 'PMMA.92', 'PMMA.93', 'PMMA.94',\n",
    "    'PMMA.105', 'PMMA.106', 'PMMA.107', 'PMMA.108', 'PMMA.109',\n",
    "    'PMMA.110', 'PMMA.111', 'PMMA.112', 'PMMA.113',\n",
    "    'PMMA.25', 'PMMA.26', 'PMMA.27',\n",
    "    'PMMA.25.1', 'PMMA.26.1', 'PMMA.27.1',\n",
    "    'PMMA.28', 'PMMA.29',\n",
    "    'PLA', 'PLA.01', 'PLA.02', 'PLA.03', 'PLA.04',\n",
    "    'PS.06', 'PS.07', 'PS.08', 'PS.09', 'PS.10',\n",
    "]\n",
    "\n",
    "# \u2500\u2500 Measurement outliers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Individual spectra removed due to measurement anomalies (not colour-related).\n",
    "# PET.12: clear PET_30PCR \u2014 only 1 of 15 readings from the same material that\n",
    "#         measured as 'unknown'; likely a one-off sensor/placement error.\n",
    "MEASUREMENT_OUTLIERS = [\n",
    "    'PET.12',\n",
    "]\n",
    "\n",
    "# \u2500\u2500 Run Task 3.02 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "ORIGINAL_DATA_PATH = os.path.join(DATA_DIR, 'matoha-data_3.csv')\n",
    "outlier_checked_path = os.path.join(OUTPUT_DIR, '3.02_outlier-checked-data.csv')\n",
    "outlier_checked_df = plot_heatmaps_by_colour(\n",
    "    checked_path,\n",
    "    outlier_checked_path,\n",
    "    ORIGINAL_DATA_PATH,\n",
    "    dark_colours={'black'},\n",
    "    manual_dark=MANUAL_DARK_OVERRIDES,\n",
    "    measurement_outliers=MEASUREMENT_OUTLIERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the heatmaps and colour separation results:**\n",
    "\n",
    "The heatmaps above show each polymer group with separated spectra (dark, metallic, or anomalous) plotted below in a **magma** colour scale, alongside the main spectra on top in the standard **viridis** scale. Labels include the recorded colour and sample description where available from the PlasTell metadata.\n",
    "\n",
    "#### What to look for when verifying the separation\n",
    "\n",
    "For each polymer group with separated spectra, check:\n",
    "\n",
    "1. **Do the separated spectra have flattened or shifted absorption features?** \u2014 Compare the lower heatmap to the upper. If the separated spectra show a uniform/washed-out pattern or peaks in completely different positions, the separation is justified.\n",
    "\n",
    "2. **Is there a clear pattern difference?** \u2014 After min-max normalisation, shape differences become apparent: dark spectra often show noise-dominated patterns, while metallic/grade-variant spectra show systematically shifted peaks.\n",
    "\n",
    "3. **Does the sample description match?** \u2014 The labels include the recorded colour and sample description (e.g., `HDPE.10 [black] \u2014 HDPE_black100mLbottle`). Cross-reference these with the physical samples to confirm.\n",
    "\n",
    "#### Why these spectra were separated\n",
    "\n",
    "The separated spectra fall into three categories:\n",
    "\n",
    "| Category | Examples | Cause | Evidence |\n",
    "|---|---|---|---|\n",
    "| **Dark/black plastics** | HDPE black bottles, PVC black tubes, PP black squares | Carbon black absorbs broadly across NIR, suppressing all polymer-specific features | All measured as `unknown` by PlasTell |\n",
    "| **Grey plastics** | PLA printed grey, HIPS grey | Pigment partially absorbs NIR, reducing feature contrast | All measured as `unknown` |\n",
    "| **Metallic/grade variants** | PMMA gold, PMMA blocks from data_source2 with shifted peaks | Metallic surface finish disrupts reflectance; different PMMA grades have different additive profiles | PMMA gold: 5/6 measured `unknown`; data_source2 blocks show systematically different peak positions |\n",
    "\n",
    "Additionally, spectra from `data_source2` (no colour metadata) were separated based on **visual inspection** of the heatmaps \u2014 these are flagged in the `MANUAL_DARK_OVERRIDES` list and can be adjusted as needed.\n",
    "\n",
    "#### Classification strategy\n",
    "\n",
    "**Primary approach (Option 1): Train on light spectra only**\n",
    "\n",
    "The cleaned dataset (with dark/separated spectra removed) is used for all subsequent classification tasks. This gives the best classification accuracy because:\n",
    "- The remaining spectra have clear, distinct polymer-specific absorption features.\n",
    "- Within-class variance is reduced \u2014 the classifier is not confused by featureless or shifted spectra.\n",
    "- The PlasTell device itself could not identify most of the separated samples (`measured = unknown`), confirming that the NIR features needed for classification are absent or distorted.\n",
    "\n",
    "**Extension (Option 2): Two-stage classifier for dark plastics**\n",
    "\n",
    "As a separate investigation, a two-stage classification approach could be explored:\n",
    "1. **Stage 1 \u2014 Light vs dark detection:** Classify whether a spectrum comes from a light or dark sample. This is a relatively easy binary classification based on spectral flatness (standard deviation across wavelengths) or overall signal intensity, since dark spectra are systematically different from light ones.\n",
    "2. **Stage 2a \u2014 Polymer classification (light):** The primary classifier trained on light spectra, as implemented in this notebook.\n",
    "3. **Stage 2b \u2014 Polymer classification (dark):** A separate classifier trained only on the dark/separated spectra, to see whether enough residual spectral features remain for polymer identification despite the pigment interference.\n",
    "\n",
    "The key question for Stage 2b is whether dark spectra retain **any** polymer-specific information. Carbon black absorption is broadband but not perfectly flat \u2014 subtle differences in the residual spectral shape *may* allow some discrimination, particularly between polymers with very different molecular structures (e.g., PET vs PP). However, with only 5\u201310 dark spectra per polymer group, training data is limited. This would require careful cross-validation and likely benefit from data augmentation or transfer learning techniques.\n",
    "\n",
    "This two-stage approach is worth investigating because real-world plastic waste streams contain a significant proportion of dark/black items. A system that can only classify light-coloured plastics has limited practical utility in recycling applications.\n",
    "\n",
    "The cleaned dataset (light spectra only) has been saved for use in the subsequent classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Dark Spectra Distinguishable? Evaluating Dark Plastic Classification Potential\n",
    "\n",
    "Before discarding the dark spectra entirely, we should examine whether they retain any polymer-specific features that could enable classification. If the dark spectra from different polymers are distinct from each other, a dedicated dark-plastic classifier may be viable. If they are indistinguishable, the dark spectra carry no useful information and can be safely excluded.\n",
    "\n",
    "Three analyses are performed:\n",
    "\n",
    "1. **Mean dark spectra overlay** \u2014 Plots the average spectrum per polymer (dark samples only) on the same axes. Distinct peaks at different wavelengths suggest polymer-specific information survives.\n",
    "2. **PCA projection** \u2014 Projects all dark spectra into 2D principal component space. If spectra from the same polymer cluster together and separate from other polymers, classification is feasible.\n",
    "3. **Inter-class similarity matrix** \u2014 Computes the Pearson correlation between each pair of dark polymer group means. High correlation (close to 1.0) between all groups means they are indistinguishable; low correlation means distinct features exist."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyse_dark_spectra(checked_path, outlier_checked_path):\n",
    "    \"\"\"Analyse whether dark spectra retain polymer-specific features.\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # \u2500\u2500 Extract dark spectra \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    df_all = pd.read_csv(checked_path)\n",
    "    df_light = pd.read_csv(outlier_checked_path)\n",
    "    \n",
    "    dark_cols = [c for c in df_all.columns[1:] if c not in df_light.columns]\n",
    "    if len(dark_cols) == 0:\n",
    "        print(\"No dark spectra found.\")\n",
    "        return\n",
    "    \n",
    "    wavelengths = df_all.iloc[:, 0].values\n",
    "    X_dark = df_all[dark_cols].apply(pd.to_numeric, errors='coerce').values.T\n",
    "    y_dark = np.array([extract_prefix(c) for c in dark_cols])\n",
    "    polymers = sorted(set(y_dark))\n",
    "    \n",
    "    print(f\"Dark spectra: {len(dark_cols)} across {len(polymers)} polymer types\")\n",
    "    print(f\"Polymers: {', '.join(polymers)}\\n\")\n",
    "    \n",
    "    # \u2500\u2500 1. Mean dark spectra overlay \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    colours_cycle = plt.cm.Set1(np.linspace(0, 1, len(polymers)))\n",
    "    mean_spectra = {}\n",
    "    \n",
    "    for i, polymer in enumerate(polymers):\n",
    "        mask = y_dark == polymer\n",
    "        spectra = X_dark[mask]\n",
    "        mean_spec = np.mean(spectra, axis=0)\n",
    "        std_spec = np.std(spectra, axis=0)\n",
    "        mean_spectra[polymer] = mean_spec\n",
    "        \n",
    "        axes[0].plot(wavelengths, mean_spec, label=f'{polymer} (n={mask.sum()})',\n",
    "                     color=colours_cycle[i], linewidth=2)\n",
    "        axes[0].fill_between(wavelengths, mean_spec - std_spec, mean_spec + std_spec,\n",
    "                             alpha=0.15, color=colours_cycle[i])\n",
    "    \n",
    "    axes[0].set_title('Mean Dark Spectra per Polymer (\u00b1 1 std)')\n",
    "    axes[0].set_xlabel('Wavelength (nm)')\n",
    "    axes[0].set_ylabel('Raw Intensity')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Also plot normalised (min-max) to compare shapes\n",
    "    for i, polymer in enumerate(polymers):\n",
    "        spec = mean_spectra[polymer]\n",
    "        spec_norm = (spec - spec.min()) / (spec.max() - spec.min()) if spec.max() > spec.min() else spec * 0\n",
    "        axes[1].plot(wavelengths, spec_norm, label=polymer,\n",
    "                     color=colours_cycle[i], linewidth=2)\n",
    "    \n",
    "    axes[1].set_title('Mean Dark Spectra \u2014 Min-Max Normalised (shape comparison)')\n",
    "    axes[1].set_xlabel('Wavelength (nm)')\n",
    "    axes[1].set_ylabel('Normalised Intensity')\n",
    "    axes[1].legend(fontsize=8)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # \u2500\u2500 2. PCA projection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    pca = PCA(n_components=2)\n",
    "    scores = pca.fit_transform(X_dark)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    for i, polymer in enumerate(polymers):\n",
    "        mask = y_dark == polymer\n",
    "        ax.scatter(scores[mask, 0], scores[mask, 1],\n",
    "                   label=f'{polymer} (n={mask.sum()})',\n",
    "                   color=colours_cycle[i], s=60, alpha=0.8, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_title(f'PCA of Dark Spectra (PC1: {pca.explained_variance_ratio_[0]*100:.1f}%, PC2: {pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # \u2500\u2500 3. Inter-class similarity matrix \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    from scipy.stats import pearsonr\n",
    "    \n",
    "    n_poly = len(polymers)\n",
    "    sim_matrix = np.zeros((n_poly, n_poly))\n",
    "    \n",
    "    for i, p1 in enumerate(polymers):\n",
    "        for j, p2 in enumerate(polymers):\n",
    "            corr, _ = pearsonr(mean_spectra[p1], mean_spectra[p2])\n",
    "            sim_matrix[i, j] = corr\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    sns.heatmap(sim_matrix, xticklabels=polymers, yticklabels=polymers,\n",
    "                annot=True, fmt='.3f', cmap='RdYlGn_r', vmin=0.5, vmax=1.0,\n",
    "                ax=ax, square=True)\n",
    "    ax.set_title('Pearson Correlation Between Mean Dark Spectra')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # \u2500\u2500 Summary statistics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Off-diagonal correlations\n",
    "    off_diag = sim_matrix[np.triu_indices(n_poly, k=1)]\n",
    "    print(f\"\\nInter-class similarity (off-diagonal correlations):\")\n",
    "    print(f\"  Mean: {off_diag.mean():.3f}\")\n",
    "    print(f\"  Min:  {off_diag.min():.3f} ({polymers[np.unravel_index(np.argmin(sim_matrix + np.eye(n_poly)*999), sim_matrix.shape)[0]]} vs {polymers[np.unravel_index(np.argmin(sim_matrix + np.eye(n_poly)*999), sim_matrix.shape)[1]]})\")\n",
    "    print(f\"  Max:  {off_diag.max():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    if off_diag.mean() > 0.95:\n",
    "        print(\"CONCLUSION: Dark spectra are highly similar across polymers (mean r > 0.95).\")\n",
    "        print(\"Polymer-specific features are largely suppressed. A dedicated dark classifier\")\n",
    "        print(\"would struggle to distinguish between polymer types reliably.\")\n",
    "    elif off_diag.mean() > 0.85:\n",
    "        print(\"CONCLUSION: Dark spectra show moderate similarity (mean r = 0.85-0.95).\")\n",
    "        print(\"Some polymer-specific features survive, but significant overlap exists.\")\n",
    "        print(\"A dark classifier may achieve partial identification with enough training data.\")\n",
    "    else:\n",
    "        print(\"CONCLUSION: Dark spectra retain distinct features (mean r < 0.85).\")\n",
    "        print(\"Different polymers produce distinguishable dark spectra \u2014 a dedicated\")\n",
    "        print(\"classifier is likely viable with sufficient training data.\")\n",
    "\n",
    "\n",
    "# \u2500\u2500 Run the analysis \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "analyse_dark_spectra(checked_path, outlier_checked_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the dark spectra analysis:**\n",
    "\n",
    "- **Mean spectra overlay (left):** If the raw intensity curves overlap heavily with no clear separation, the dark spectra carry little polymer-specific information. If distinct peaks appear at different wavelengths for different polymers, some classification potential exists. The \u00b11 std shading shows within-group consistency \u2014 tight bands suggest reliable features.\n",
    "\n",
    "- **Normalised shape comparison (right):** By removing intensity differences, this shows whether the *shape* of the spectral profile differs between polymers. If all normalised curves follow the same pattern, the dark spectra are truly indistinguishable. If the curves diverge in specific wavelength regions, those regions carry discriminative information.\n",
    "\n",
    "- **PCA scatter:** Tight, well-separated clusters indicate that a classifier could distinguish dark polymer types. Overlapping clusters suggest the features are too similar. The percentage of variance explained by PC1 and PC2 indicates how much structure exists in the data \u2014 if two components explain >80% of variance, the dark spectra have relatively simple structure (few distinguishing features).\n",
    "\n",
    "- **Similarity matrix:** Correlation values close to 1.0 between different polymers confirm they are spectrally indistinguishable. Values below 0.9 suggest meaningful differences that a classifier could exploit. The most dissimilar pairs represent the easiest classification problems for dark spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 4: Classifying Your Data using a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.01: Spectral Data Classification Using k-Nearest Neighbours (kNN)\n\n**Objective:** Train a kNN classifier to identify polymers from their NIR spectra.\n\n**How kNN works (plain English):**\n1. You have a new unknown spectrum and want to know which polymer it is.\n2. kNN looks at the `k` most similar spectra in the training set (the \"nearest neighbours\").\n3. Whatever polymer type appears most among those `k` neighbours becomes the prediction.\n4. \"Similarity\" is measured by **Euclidean distance** \u2014 essentially, how close two spectra are when treated as points in 128-dimensional space (one dimension per wavelength).\n\n**Key steps in the pipeline:**\n\n| Step | What it does | Why |\n|---|---|---|\n| `train_test_split(test_size=0.5)` | Randomly splits data 50/50 into training and test sets | We train on one half and evaluate on the other \u2014 this tests whether the model generalises to unseen data |\n| `stratify=y` | Ensures the split keeps the same polymer proportions in both halves | Without this, the test set might randomly have no ABS samples |\n| `random_state=42` | Fixes the random split so results are reproducible | Same reason as `np.random.seed(42)` in the setup cell |\n| `StandardScaler` | Centres each feature to mean=0 and scales to std=1 | kNN uses distances \u2014 if one wavelength has values ~60000 and another ~100, the first would dominate the distance calculation unfairly. Scaling makes all wavelengths equally important |\n| `fit_transform` vs `transform` | `fit_transform` on training data learns the mean/std, then scales. `transform` on test data uses the *same* mean/std | Critical: the test set must be scaled using training statistics, not its own \u2014 otherwise you're \"peeking\" at test data (data leakage) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_spectral_data(filepath):\n    \"\"\"Load a spectral CSV and convert it into ML-ready format.\n    \n    Input:  CSV with wavelengths as rows, spectra as columns\n    Output: X = feature matrix (n_samples x 128 features)\n            y = label array (polymer name for each sample)\n            wavelengths = the 128 wavelength values\n    \n    The key transformation is the TRANSPOSE (.T): in the CSV, each column\n    is one spectrum (128 rows). But ML expects each ROW to be one sample.\n    So we flip the matrix: (128 wavelengths x n_spectra) \u2192 (n_spectra x 128 features).\n    \"\"\"\n    df = pd.read_csv(filepath)\n    wavelengths = df.iloc[:, 0].values       # The wavelength column (1550-1950 nm)\n    spectra_columns = df.columns[1:]         # Every other column is a spectrum\n    \n    # Transpose: each spectrum (column) becomes a row (sample)\n    # .apply(pd.to_numeric, errors='coerce') ensures everything is a number\n    X = df[spectra_columns].apply(pd.to_numeric, errors='coerce').values.T\n    \n    # Extract the polymer label from each column name\n    y = np.array([extract_prefix(col) for col in spectra_columns])\n    \n    return X, y, wavelengths\n\n\ndef run_knn_classification(filepath, k=5, test_size=0.5):\n    \"\"\"Train and evaluate a standard kNN classifier on spectral data.\n    \n    Pipeline: load data \u2192 split \u2192 scale \u2192 train kNN \u2192 predict \u2192 evaluate\n    \"\"\"\n    X, y, _ = prepare_spectral_data(filepath)\n    \n    # Split data: 50% for training, 50% for testing\n    # stratify=y ensures each polymer is proportionally represented in both sets\n    # random_state=42 makes the split reproducible (same split every time)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42, stratify=y\n    )\n    \n    # StandardScaler: centre each of the 128 wavelength features to mean=0, std=1\n    # This is ESSENTIAL for kNN because it uses Euclidean distance \u2014 without scaling,\n    # wavelengths with larger raw values would dominate the distance calculation\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)  # Learn mean/std from training data, then scale it\n    X_test_scaled = scaler.transform(X_test)         # Scale test data using the SAME mean/std (no peeking!)\n    \n    # Train the kNN model: k=5 means \"look at the 5 nearest neighbours\"\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)   # \"Fitting\" kNN just stores the training data \u2014 it's a lazy learner\n    \n    # Predict polymer labels for the test set\n    y_pred = knn.predict(X_test_scaled)\n    \n    # Print evaluation metrics\n    print(f'Occurrences of each polymer type in the test set:')\n    test_counts = pd.Series(y_test).value_counts()\n    print(test_counts)\n    \n    print(f'\\nClassification Report (k={k}):')\n    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n    \n    # Confusion matrix: rows = actual polymer, columns = predicted polymer\n    # Diagonal = correct predictions, off-diagonal = misclassifications\n    cm = confusion_matrix(y_test, y_pred, labels=labels)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('Predicted Label')\n    ax.set_ylabel('True Label')\n    ax.set_title(f'Confusion Matrix (kNN, k={k})')\n    plt.tight_layout()\n    plt.show()\n    \n    return knn, scaler\n\n\n# \u2500\u2500 Run Task 4.01 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Train kNN with k=5 on the cleaned (post-outlier) dataset\nknn_model, knn_scaler = run_knn_classification(outlier_checked_path, k=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**How to read the classification report:**\n\n| Metric | What it means | Example |\n|---|---|---|\n| **Precision** | \"Of all the times the model said 'PET', what fraction was actually PET?\" | Precision = 0.80 means 80% of PET predictions were correct, 20% were wrong |\n| **Recall** | \"Of all the actual PET samples, what fraction did the model correctly identify?\" | Recall = 0.90 means the model found 90% of PET samples, missed 10% |\n| **F1-score** | The harmonic mean of precision and recall \u2014 a single number balancing both | High F1 = good at both finding the polymer AND not mislabelling others as it |\n| **Support** | How many test samples exist for that polymer | Low support (e.g., 2-3) means the metric is unreliable for that class |\n\n**How to read the confusion matrix:**\n- Each **row** = the actual (true) polymer type.\n- Each **column** = what the model predicted.\n- **Diagonal cells** (top-left to bottom-right) = correct predictions.\n- **Off-diagonal cells** = mistakes. For example, a \"3\" in row PVC / column PMMA means the model misidentified 3 PVC spectra as PMMA."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.02: Building a Weighted-kNN Model\n\n**Objective:** Improve on standard kNN with two enhancements: **distance weighting** and a **certainty threshold**.\n\n**Problem with standard kNN:** All `k` neighbours get an equal vote, regardless of how close or far they are. A very distant neighbour (which might belong to a different class) counts just as much as the closest neighbour.\n\n**Enhancement 1 \u2014 Distance weighting (`weights='distance'`):**\n- Closer neighbours get a **stronger vote** than distant ones.\n- Technically, each neighbour's vote is weighted by `1/distance` \u2014 so a neighbour at distance 0.1 has 10x more influence than one at distance 1.0.\n- This makes the model less sensitive to outliers lurking at the edges of clusters.\n\n**Enhancement 2 \u2014 Certainty threshold (0.6):**\n- After predicting, the model checks its **confidence** (the fraction of weighted votes for the winning class).\n- If confidence is below 60%, the prediction is labelled **\"Uncertain\"** instead of forcing a potentially wrong answer.\n- **Why this matters for polymer recycling:** Misidentifying a polymer contaminates the recycling stream. It's better to flag a sample as \"uncertain\" and re-test it than to confidently give the wrong answer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_weighted_knn(filepath, k=5, test_size=0.5, certainty_threshold=0.6):\n",
    "    \"\"\"Train and evaluate a distance-weighted kNN with a certainty threshold.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Weighted kNN: weights='distance' gives closer neighbours more influence\n",
    "    wknn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    wknn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict with probabilities\n",
    "    y_proba = wknn.predict_proba(X_test_scaled)\n",
    "    y_pred_raw = wknn.predict(X_test_scaled)\n",
    "    certainties = np.max(y_proba, axis=1)\n",
    "    \n",
    "    # Apply certainty threshold\n",
    "    y_pred = np.where(certainties >= certainty_threshold, y_pred_raw, 'Uncertain')\n",
    "    \n",
    "    # Counts\n",
    "    print('Occurrences of each polymer type in the test set:')\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    print(f'\\nPrediction counts (including Uncertain):')\n",
    "    print(pd.Series(y_pred).value_counts())\n",
    "    \n",
    "    # Classification report\n",
    "    all_labels = sorted(set(list(y_test) + list(y_pred)))\n",
    "    print(f'\\nClassification Report (Weighted kNN, k={k}, threshold={certainty_threshold}):')\n",
    "    print(classification_report(y_test, y_pred, labels=all_labels, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_labels)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=all_labels, yticklabels=all_labels, ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    axes[0].set_title(f'Confusion Matrix (Weighted kNN, k={k})')\n",
    "    \n",
    "    # Certainty histogram\n",
    "    axes[1].hist(certainties, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=certainty_threshold, color='red', linestyle='--',\n",
    "                    label=f'Threshold = {certainty_threshold}')\n",
    "    axes[1].set_xlabel('Prediction Certainty')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of Prediction Certainties')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return wknn, scaler\n",
    "\n",
    "\n",
    "# Run Task 4.02\n",
    "wknn_model, wknn_scaler = run_weighted_knn(outlier_checked_path, k=5, certainty_threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The certainty histogram shows how confident the model is across all test predictions. Predictions below the threshold are labelled \"Uncertain\" \u2014 this is preferable to misidentification in a real-world recycling context, where misidentified polymers could contaminate the recycling stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.03: Extension \u2014 Grid Search and SVM\n\n**Objective:** Try two additional approaches to improve classification:\n\n**1. Grid search for optimal `k`:**\n- So far we've used k=5 (look at 5 neighbours), but is 5 the best choice? Maybe k=3 or k=11 works better.\n- **Grid search** systematically tries every combination of `k` from 1 to 20 and `weights` (uniform vs distance), evaluates each using **5-fold cross-validation** (splits the training data 5 ways and averages performance), and picks the best combination.\n- `scoring='f1_macro'` means we optimise for the macro-averaged F1 score \u2014 this gives equal importance to all polymer classes regardless of how many spectra they have.\n\n**2. Support Vector Machine (SVM) as an alternative classifier:**\n- SVM works differently from kNN: instead of finding nearest neighbours, it finds the **optimal boundary** (hyperplane) that separates the polymer classes in feature space.\n- The **RBF (Radial Basis Function) kernel** allows SVM to draw curved (non-linear) boundaries, which can better separate classes that aren't linearly separable in the 128-dimensional wavelength space.\n- Comparing SVM to kNN tells us whether a fundamentally different algorithm performs better on this data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def grid_search_knn(filepath, test_size=0.5):\n",
    "    \"\"\"Find optimal k for kNN using grid search with cross-validation.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Grid search over k values\n",
    "    param_grid = {'n_neighbors': list(range(1, 21)), 'weights': ['uniform', 'distance']}\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='f1_macro')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f'Best parameters: {grid.best_params_}')\n",
    "    print(f'Best cross-validation F1 score: {grid.best_score_:.3f}')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = grid.predict(X_test_scaled)\n",
    "    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    print(f'\\nTest set Classification Report (optimised kNN):')\n",
    "    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def run_svm_classification(filepath, test_size=0.5):\n",
    "    \"\"\"Train and evaluate an SVM classifier for comparison.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    \n",
    "    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    print('SVM Classification Report:')\n",
    "    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_title('Confusion Matrix (SVM, RBF kernel)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return svm\n",
    "\n",
    "\n",
    "# Run grid search\n",
    "print('=== Grid Search for Optimal kNN Parameters ===')\n",
    "best_knn = grid_search_knn(outlier_checked_path)\n",
    "\n",
    "print('\\n=== SVM Classifier ===')\n",
    "svm_model = run_svm_classification(outlier_checked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the optimised kNN, weighted kNN, and SVM results helps determine which model is best suited for this spectral classification task. SVM with an RBF kernel can capture non-linear decision boundaries, which may improve classification of spectrally similar polymers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Model Performance: Before vs After Colour-Based Separation\n",
    "\n",
    "**Objective:** Quantify the impact of the colour-based separation step (Task 3.02) on classification accuracy. Three models \u2014 standard kNN (k=5), distance-weighted kNN (k=5), and SVM (RBF kernel) \u2014 are trained and evaluated on both the pre-separation dataset (`2.07_checked-data.csv`, containing all spectra including dark/metallic samples) and the post-separation dataset (`3.02_outlier-checked-data.csv`, light spectra only). Overall accuracy and macro-averaged F1-score are compared side-by-side to determine whether separating dark and anomalous spectra improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_models_on_dataset(filepath, dataset_label):\n",
    "    \"\"\"Train kNN, weighted kNN, and SVM on a dataset; return accuracy, macro F1, and per-class F1.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    class_labels = sorted(np.unique(y))\n",
    "    results = []\n",
    "\n",
    "    models = [\n",
    "        ('kNN (k=5)', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('Weighted kNN (k=5)', KNeighborsClassifier(n_neighbors=5, weights='distance')),\n",
    "        ('SVM (RBF)', SVC(kernel='rbf', random_state=42)),\n",
    "    ]\n",
    "\n",
    "    for model_name, model in models:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        row = {\n",
    "            'Dataset': dataset_label,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Macro F1': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        }\n",
    "\n",
    "        # Per-class F1 scores\n",
    "        per_class = f1_score(y_test, y_pred, labels=class_labels, average=None, zero_division=0)\n",
    "        for cls, score in zip(class_labels, per_class):\n",
    "            row[f'F1 {cls}'] = score\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return results, class_labels\n",
    "\n",
    "\n",
    "# Evaluate on both datasets\n",
    "before_results, class_labels_before = evaluate_models_on_dataset(checked_path, 'Before Outlier Removal')\n",
    "after_results, class_labels_after = evaluate_models_on_dataset(outlier_checked_path, 'After Outlier Removal')\n",
    "\n",
    "# Combine into a comparison DataFrame\n",
    "comparison_df = pd.DataFrame(before_results + after_results)\n",
    "\n",
    "# Identify per-class F1 columns\n",
    "all_class_labels = sorted(set(class_labels_before) | set(class_labels_after))\n",
    "per_class_cols = [f'F1 {cls}' for cls in all_class_labels]\n",
    "\n",
    "# Format numeric columns\n",
    "for col in ['Accuracy', 'Macro F1'] + per_class_cols:\n",
    "    if col in comparison_df.columns:\n",
    "        comparison_df[col] = comparison_df[col].map('{:.3f}'.format)\n",
    "\n",
    "# --- Summary table (Accuracy + Macro F1) ---\n",
    "summary_cols = ['Dataset', 'Model', 'Accuracy', 'Macro F1']\n",
    "pivot_acc = comparison_df.pivot(index='Model', columns='Dataset', values='Accuracy')\n",
    "pivot_f1 = comparison_df.pivot(index='Model', columns='Dataset', values='Macro F1')\n",
    "\n",
    "print('=== Accuracy Comparison ===')\n",
    "print(pivot_acc[['Before Outlier Removal', 'After Outlier Removal']].to_string())\n",
    "print()\n",
    "print('=== Macro-Averaged F1 Comparison ===')\n",
    "print(pivot_f1[['Before Outlier Removal', 'After Outlier Removal']].to_string())\n",
    "print()\n",
    "\n",
    "# --- Per-class F1 table ---\n",
    "print('=== Per-Class F1 Scores ===')\n",
    "display_cols = ['Dataset', 'Model'] + per_class_cols\n",
    "print(comparison_df[display_cols].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# --- Full table ---\n",
    "print('=== Full Comparison Table ===')\n",
    "print(comparison_df.to_string(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "The tables above show how each model's accuracy, macro-averaged F1-score, and per-class F1-scores change after removing dark, metallic, and anomalous spectra. Key observations:\n",
    "\n",
    "- If the **After** scores are higher, the colour-based separation successfully removed spectra whose suppressed or shifted features were confusing the classifiers. Dark plastics in particular look similar across different polymer types (all flat/featureless), which creates overlapping decision boundaries.\n",
    "- If the scores are similar or slightly lower, the smaller dataset may have reduced the model's ability to generalise \u2014 though the remaining spectra should be more internally consistent.\n",
    "- The **macro-averaged F1** is particularly informative for imbalanced datasets because it weights all polymer classes equally, preventing high-frequency classes (e.g., PMMA with 115+ spectra) from masking poor performance on rare ones (e.g., ABS with ~20 spectra).\n",
    "- The **per-class F1 scores** reveal which specific polymer types benefit most from the separation. Polymers that had dark spectra removed (HDPE, PET, PP, PVC, PS, PLA, PMMA) should show the largest improvements, since their dark samples would have distorted the decision boundaries.\n",
    "- Among the three models, SVM (RBF) typically benefits the most from cleaning because support vectors near the decision boundary are sensitive to noisy or anomalous data points.\n",
    "\n",
    "This comparison validates the physically motivated colour-based separation approach: rather than applying blind statistical methods, we identified and removed spectra where the NIR measurement physics itself prevents reliable polymer identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 5: Identifying Your Unknown Polymers\n",
    "\n",
    "**Objective:** Use the best-performing trained model to classify the 10 unknown polymer samples. The unknown spectra must first be processed through the same pipeline (transform, check for invalid values) before being fed to the model.\n",
    "\n",
    "*Update the `UNKNOWN_CSV` path below once you have collected the unknown spectra using PlasTell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: update this path with your actual unknown samples export\n",
    "UNKNOWN_CSV = os.path.join(DATA_DIR, 'unknown_samples.csv')  # UPDATE THIS\n",
    "\n",
    "\n",
    "def identify_unknowns(model, scaler, unknown_filepath, reference_filepath):\n",
    "    \"\"\"Process unknown spectra and predict polymer identity.\"\"\"\n",
    "    if not os.path.exists(unknown_filepath):\n",
    "        print(f'Unknown samples file not found: {unknown_filepath}')\n",
    "        print('Please collect unknown spectra and update the UNKNOWN_CSV path.')\n",
    "        return None\n",
    "    \n",
    "    # Load and transform unknown data (same pipeline as training data)\n",
    "    unknown_raw = pd.read_csv(unknown_filepath)\n",
    "    unknown_raw['spectrum'] = unknown_raw['spectrum'].apply(ast.literal_eval)\n",
    "    \n",
    "    X_unknown = np.array(unknown_raw['spectrum'].tolist())\n",
    "    X_unknown_scaled = scaler.transform(X_unknown)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_unknown_scaled)\n",
    "    \n",
    "    # If model supports predict_proba, show certainties\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probas = model.predict_proba(X_unknown_scaled)\n",
    "        certainties = np.max(probas, axis=1)\n",
    "    else:\n",
    "        certainties = ['N/A'] * len(predictions)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Sample': range(1, len(predictions) + 1),\n",
    "        'Predicted Polymer': predictions,\n",
    "        'Certainty': certainties\n",
    "    })\n",
    "    \n",
    "    print('Unknown Sample Predictions:')\n",
    "    print(results.to_string(index=False))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run Task 5 (will print guidance if unknown file doesn't exist yet)\n",
    "unknown_results = identify_unknowns(wknn_model, wknn_scaler, UNKNOWN_CSV, outlier_checked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical assessment:** When evaluating the model's predictions on unknown samples, consider:\n",
    "- Are the certainty values consistently high? Low certainty may indicate the unknown polymer is not well-represented in the training data.\n",
    "- Could the unknown polymer be a type not present in the reference database? kNN will always assign one of the known classes, even if the true answer is none of them.\n",
    "- Physical/chemical factors such as polymer colour, thickness, additives, and surface texture can all influence the NIR spectrum and reduce classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n### PCA Visualisation of Polymer Feature Space\n\n**Principal Component Analysis (PCA)** is a dimensionality reduction technique that projects high-dimensional data (here, 128 wavelength features per spectrum) onto a smaller set of orthogonal axes called **principal components**. Each component captures the maximum remaining variance in the data.\n\nFor NIR spectral data, PCA is useful because:\n- It reveals the **natural clustering** of polymer types in a reduced feature space.\n- It shows whether polymers that are spectrally similar (and therefore harder to classify) overlap in the low-dimensional projection.\n- The **scree plot** indicates how many components are needed to capture most of the variance \u2014 if the first 2\u20133 components explain >90% of variance, the spectral differences between polymers are dominated by a few key absorption features.\n- Overlaying outlier spectra on the PCA projection shows whether removed outliers genuinely fall outside the main clusters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u2500\u2500 1. Load cleaned (post-outlier) data and fit PCA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nX_clean, y_clean, wl_clean = prepare_spectral_data(outlier_checked_path)\n\nscaler_pca = StandardScaler()\nX_clean_scaled = scaler_pca.fit_transform(X_clean)\n\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X_clean_scaled)\n\nprint(f'Post-outlier dataset: {X_clean.shape[0]} spectra, {X_clean.shape[1]} features')\nprint(f'Explained variance (first 10 PCs): {pca.explained_variance_ratio_.sum():.1%}')\nprint(f'Per-component: {[f\"{v:.1%}\" for v in pca.explained_variance_ratio_]}')\n\n# \u2500\u2500 2. Scree plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, ax = plt.subplots(figsize=(10, 5))\ncomponents = np.arange(1, 11)\nbars = ax.bar(components, pca.explained_variance_ratio_ * 100, color='steelblue',\n              edgecolor='black', alpha=0.8, label='Individual')\ncumulative = np.cumsum(pca.explained_variance_ratio_) * 100\nax.plot(components, cumulative, 'ro-', linewidth=2, label='Cumulative')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance (%)')\nax.set_title('Scree Plot \u2014 Explained Variance by Principal Component')\nax.set_xticks(components)\nax.legend(loc='center right')\nax.set_ylim(0, 105)\nfor i, (bar, val) in enumerate(zip(bars, pca.explained_variance_ratio_ * 100)):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n            f'{val:.1f}%', ha='center', va='bottom', fontsize=8)\nplt.tight_layout()\nplt.show()\n\n# \u2500\u2500 3. 2D scatter plot (PC1 vs PC2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nunique_labels = sorted(np.unique(y_clean))\ncmap = plt.cm.tab10\ncolors = {label: cmap(i / max(len(unique_labels) - 1, 1)) for i, label in enumerate(unique_labels)}\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfor label in unique_labels:\n    mask = y_clean == label\n    ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n               c=[colors[label]], label=label, s=50, alpha=0.7, edgecolors='k', linewidths=0.3)\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax.set_title('PCA \u2014 Polymer Spectra Projected onto PC1 vs PC2')\nax.legend(title='Polymer', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\nplt.tight_layout()\nplt.show()\n\n# \u2500\u2500 4. 3D scatter plot (PC1 vs PC2 vs PC3) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig = plt.figure(figsize=(11, 8))\nax3d = fig.add_subplot(111, projection='3d')\nfor label in unique_labels:\n    mask = y_clean == label\n    ax3d.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n                 c=[colors[label]], label=label, s=40, alpha=0.7, edgecolors='k', linewidths=0.3)\nax3d.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax3d.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax3d.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\nax3d.set_title('PCA \u2014 3D Projection (PC1, PC2, PC3)')\nax3d.legend(title='Polymer', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\nplt.tight_layout()\nplt.show()\n\n# \u2500\u2500 5. Overlay plot: pre-outlier data with outliers highlighted \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nX_pre, y_pre, _ = prepare_spectral_data(checked_path)\nX_post, y_post, _ = prepare_spectral_data(outlier_checked_path)\n\n# Identify outlier columns: present in pre-outlier but absent from post-outlier\ndf_pre = pd.read_csv(checked_path)\ndf_post = pd.read_csv(outlier_checked_path)\npre_cols = set(df_pre.columns[1:])\npost_cols = set(df_post.columns[1:])\noutlier_col_names = pre_cols - post_cols\nprint(f'Outlier spectra identified: {len(outlier_col_names)}')\nif outlier_col_names:\n    print(f'  Columns: {sorted(outlier_col_names)}')\n\n# Scale ALL pre-outlier data using the same scaler fitted on clean data\nX_pre_scaled = scaler_pca.transform(X_pre)\nX_pre_pca = pca.transform(X_pre_scaled)\n\n# Build mask: True for outlier spectra\npre_col_list = list(df_pre.columns[1:])\noutlier_mask = np.array([col in outlier_col_names for col in pre_col_list])\nnormal_mask = ~outlier_mask\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot normal (non-outlier) points coloured by polymer\nfor label in unique_labels:\n    mask = (y_pre == label) & normal_mask\n    if mask.any():\n        ax.scatter(X_pre_pca[mask, 0], X_pre_pca[mask, 1],\n                   c=[colors[label]], label=label, s=50, alpha=0.6,\n                   edgecolors='k', linewidths=0.3)\n\n# Plot outliers with red 'x' markers\nif outlier_mask.any():\n    ax.scatter(X_pre_pca[outlier_mask, 0], X_pre_pca[outlier_mask, 1],\n               c='red', marker='x', s=100, linewidths=2, zorder=5,\n               label=f'Outliers (n={outlier_mask.sum()})')\n\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax.set_title('PCA \u2014 Pre-Outlier Data with Removed Outliers Highlighted')\nax.legend(title='Polymer', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the PCA results:**\n\n- **Scree plot:** The first few principal components typically capture the majority of spectral variance. A sharp \"elbow\" in the scree plot indicates that most meaningful variation is concentrated in just 2--3 dimensions, which is expected for NIR data where polymer identity is primarily encoded in a handful of absorption bands.\n\n- **2D scatter (PC1 vs PC2):** Well-separated clusters indicate that the corresponding polymers have distinct spectral signatures and should be easy to classify. Overlapping clusters suggest spectral similarity between those polymer types -- these are the pairs most likely to be confused by kNN or SVM classifiers (consistent with off-diagonal entries in the confusion matrices above).\n\n- **3D scatter (PC1--PC3):** Adding the third component can resolve clusters that overlap in the 2D projection. If two polymer groups overlap in PC1-PC2 but separate along PC3, this indicates that a third spectral feature distinguishes them.\n\n- **Outlier overlay:** Outlier spectra (red crosses) that fall far from their polymer's cluster confirm that the Pearson correlation-based detection in Task 3.02 correctly identified anomalous measurements. Outliers that fall *within* another polymer's cluster may represent mislabelled samples or contaminated measurements. The spatial separation between outliers and their nominal class provides visual justification for their removal from the training data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance\n",
    "\n",
    "The dataset is severely imbalanced \u2014 some polymer classes (e.g., PMMA) have many more spectra than others (e.g., ABS with only a handful). Standard kNN treats all training samples equally, so it is biased toward predicting the majority class. This means minority classes may be consistently misclassified even when the model reports high overall accuracy.\n",
    "\n",
    "Three strategies are compared below:\n",
    "1. **SMOTE (Synthetic Minority Oversampling Technique):** Generates synthetic training samples for minority classes by interpolating between existing minority-class neighbours. This balances the training set without discarding data.\n",
    "2. **Random Undersampling:** Reduces the majority class(es) by randomly removing training samples until all classes have equal representation. Simple but discards potentially useful data.\n",
    "3. **Class-Weighted kNN:** Instead of modifying the training data, sample weights inversely proportional to class frequency are used during training so that minority-class samples carry more influence.\n",
    "\n",
    "All resampling is applied **only to the training set** to avoid data leakage into the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --- Load and split the cleaned data ---\n",
    "X_imb, y_imb, _ = prepare_spectral_data(outlier_checked_path)\n",
    "\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.5, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "scaler_imb = StandardScaler()\n",
    "X_train_scaled_imb = scaler_imb.fit_transform(X_train_imb)\n",
    "X_test_scaled_imb = scaler_imb.transform(X_test_imb)\n",
    "\n",
    "print('Training set class distribution (before resampling):')\n",
    "print(pd.Series(y_train_imb).value_counts())\n",
    "print()\n",
    "\n",
    "# Store results for comparison\n",
    "results_list = []\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Baseline: standard kNN (no balancing)\n",
    "# -------------------------------------------------------------------\n",
    "knn_baseline = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_baseline.fit(X_train_scaled_imb, y_train_imb)\n",
    "y_pred_baseline = knn_baseline.predict(X_test_scaled_imb)\n",
    "\n",
    "report_baseline = classification_report(\n",
    "    y_test_imb, y_pred_baseline, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Baseline (no balancing)',\n",
    "    'Macro F1': report_baseline['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_baseline['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_baseline['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Baseline kNN (no balancing) ===')\n",
    "print(classification_report(y_test_imb, y_pred_baseline, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. SMOTE oversampling on training set\n",
    "# -------------------------------------------------------------------\n",
    "# SMOTE needs at least k_neighbors+1 samples per minority class.\n",
    "# Determine a safe k_neighbors value for SMOTE.\n",
    "min_train_count = min(Counter(y_train_imb).values())\n",
    "smote_k = min(5, min_train_count - 1) if min_train_count > 1 else 1\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=smote_k)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled_imb, y_train_imb)\n",
    "\n",
    "print(f'SMOTE resampled training set distribution (k_neighbors={smote_k}):')\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "print()\n",
    "\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = knn_smote.predict(X_test_scaled_imb)\n",
    "\n",
    "report_smote = classification_report(\n",
    "    y_test_imb, y_pred_smote, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'SMOTE oversampling',\n",
    "    'Macro F1': report_smote['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_smote['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_smote['accuracy']\n",
    "})\n",
    "\n",
    "print('=== SMOTE Oversampling ===')\n",
    "print(classification_report(y_test_imb, y_pred_smote, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Random undersampling on training set\n",
    "# -------------------------------------------------------------------\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled_imb, y_train_imb)\n",
    "\n",
    "print('Undersampled training set distribution:')\n",
    "print(pd.Series(y_train_rus).value_counts())\n",
    "print()\n",
    "\n",
    "knn_rus = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_rus.fit(X_train_rus, y_train_rus)\n",
    "y_pred_rus = knn_rus.predict(X_test_scaled_imb)\n",
    "\n",
    "report_rus = classification_report(\n",
    "    y_test_imb, y_pred_rus, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Random undersampling',\n",
    "    'Macro F1': report_rus['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_rus['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_rus['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Random Undersampling ===')\n",
    "print(classification_report(y_test_imb, y_pred_rus, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Class-weighted kNN using sample weights\n",
    "# -------------------------------------------------------------------\n",
    "# sklearn's KNeighborsClassifier does not natively support class_weight,\n",
    "# so we use a distance-weighted kNN and compute inverse-frequency sample\n",
    "# weights via compute_sample_weight('balanced', ...). We wrap the\n",
    "# approach by fitting on the original training set but weighting\n",
    "# neighbours by class frequency during prediction using a custom scorer.\n",
    "#\n",
    "# Practical approach: use compute_sample_weight to get per-sample weights,\n",
    "# then duplicate minority samples proportionally to approximate weighting.\n",
    "\n",
    "sample_weights = compute_sample_weight('balanced', y_train_imb)\n",
    "\n",
    "# Duplicate samples according to their weight (rounded to integers)\n",
    "repeat_counts = np.round(sample_weights).astype(int)\n",
    "repeat_counts = np.maximum(repeat_counts, 1)  # At least 1 copy\n",
    "\n",
    "X_train_weighted = np.repeat(X_train_scaled_imb, repeat_counts, axis=0)\n",
    "y_train_weighted = np.repeat(y_train_imb, repeat_counts, axis=0)\n",
    "\n",
    "print('Class-weighted (duplicated) training set distribution:')\n",
    "print(pd.Series(y_train_weighted).value_counts())\n",
    "print()\n",
    "\n",
    "knn_weighted = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_weighted.fit(X_train_weighted, y_train_weighted)\n",
    "y_pred_weighted = knn_weighted.predict(X_test_scaled_imb)\n",
    "\n",
    "report_weighted = classification_report(\n",
    "    y_test_imb, y_pred_weighted, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Class-weighted kNN',\n",
    "    'Macro F1': report_weighted['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_weighted['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_weighted['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Class-Weighted kNN ===')\n",
    "print(classification_report(y_test_imb, y_pred_weighted, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Comparison table\n",
    "# -------------------------------------------------------------------\n",
    "comparison_df = pd.DataFrame(results_list)\n",
    "comparison_df = comparison_df.set_index('Method')\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('CLASS IMBALANCE STRATEGY COMPARISON')\n",
    "print('=' * 60)\n",
    "print(comparison_df.to_string())\n",
    "print('=' * 60)\n",
    "\n",
    "# --- Confusion matrices side-by-side ---\n",
    "labels_imb = sorted(np.unique(np.concatenate([y_test_imb, y_pred_baseline])))\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n",
    "titles = ['Baseline (no balancing)', 'SMOTE', 'Random Undersampling', 'Class-Weighted kNN']\n",
    "predictions = [y_pred_baseline, y_pred_smote, y_pred_rus, y_pred_weighted]\n",
    "\n",
    "for ax, title, y_pred in zip(axes, titles, predictions):\n",
    "    cm = confusion_matrix(y_test_imb, y_pred, labels=labels_imb)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels_imb, yticklabels=labels_imb, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrices: Class Imbalance Strategies', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the class imbalance results:**\n",
    "\n",
    "The comparison table above shows how each balancing strategy affects classification performance:\n",
    "\n",
    "- **Macro F1** treats all classes equally regardless of size, making it the most important metric when minority-class performance matters (e.g., correctly identifying a rare polymer in a recycling stream).\n",
    "- **Weighted F1** gives more importance to larger classes, so it tends to look favourable even when minority classes are poorly classified.\n",
    "- **Accuracy** can be misleading with imbalanced data \u2014 a model that always predicts the majority class can still achieve high accuracy.\n",
    "\n",
    "**Trade-offs between approaches:**\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|---|---|---|\n",
    "| SMOTE | Preserves all real data; generates plausible synthetic samples | May create unrealistic spectra if classes overlap in feature space; requires enough minority samples to interpolate |\n",
    "| Random Undersampling | Simple; fast; no synthetic data | Discards majority-class data that may contain useful information; high variance with very small datasets |\n",
    "| Class-Weighted kNN | No data modification; accounts for imbalance during model fitting | Approximate (uses sample duplication); may over-represent noisy minority samples |\n",
    "\n",
    "In practice, the best strategy depends on the degree of imbalance and the number of available samples. When minority classes have very few spectra (e.g., fewer than 5), SMOTE may struggle because it needs neighbours to interpolate between, while undersampling would reduce the training set to an impractically small size. In such cases, class-weighted approaches or collecting additional data for under-represented polymers may be the most practical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Extension: Two-Stage Classification \u2014 Can Dark Plastics Be Identified?\n",
    "\n",
    "**Objective:** Investigate whether the dark/separated spectra retain enough residual spectral features for polymer identification, despite pigment interference suppressing the normal NIR absorption signatures.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "In real-world plastic waste streams, a significant proportion of items are dark or black. A classification system that can only identify light-coloured plastics has limited practical utility. If the dark spectra contain *any* polymer-specific information \u2014 even subtle differences \u2014 a dedicated classifier might achieve partial identification.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "1. **Stage 1 \u2014 Light vs dark detection:** Train a binary classifier to distinguish light from dark spectra. This should be straightforward since dark spectra are systematically different (flatter profiles, lower spectral variation).\n",
    "2. **Stage 2b \u2014 Dark polymer classification:** Train a separate classifier on only the dark/separated spectra to see if polymer identification is possible.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Small sample sizes: most dark polymer groups have only 5\u201315 spectra.\n",
    "- High within-class similarity: dark spectra from different polymers may look very similar once carbon black dominates.\n",
    "- Results should be interpreted cautiously \u2014 with so few samples, cross-validation scores may be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def dark_classifier_experiment(checked_path, outlier_checked_path):\n",
    "    \"\"\"Compare classification on light-only vs dark-only spectra.\n",
    "    \n",
    "    Trains kNN and SVM on both datasets to quantify how much polymer-specific\n",
    "    information remains in the dark spectra.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    # \u2500\u2500 Load both datasets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    df_all = pd.read_csv(checked_path)\n",
    "    df_light = pd.read_csv(outlier_checked_path)\n",
    "    \n",
    "    # Dark spectra = all spectra minus light spectra\n",
    "    dark_cols = [c for c in df_all.columns[1:] if c not in df_light.columns]\n",
    "    \n",
    "    if len(dark_cols) == 0:\n",
    "        print(\"No dark spectra found.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare dark dataset\n",
    "    df_dark = df_all[['Wavelength (nm)'] + dark_cols] if 'Wavelength (nm)' in df_all.columns else df_all[[df_all.columns[0]] + dark_cols]\n",
    "    \n",
    "    X_dark = df_dark.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values.T\n",
    "    y_dark = np.array([extract_prefix(c) for c in dark_cols])\n",
    "    \n",
    "    # Prepare light dataset\n",
    "    light_cols = [c for c in df_light.columns[1:]]\n",
    "    X_light = df_light.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').values.T\n",
    "    y_light = np.array([extract_prefix(c) for c in light_cols])\n",
    "    \n",
    "    print(f\"Light dataset: {X_light.shape[0]} spectra, {len(set(y_light))} polymer types\")\n",
    "    print(f\"Dark dataset:  {X_dark.shape[0]} spectra, {len(set(y_dark))} polymer types\")\n",
    "    print()\n",
    "    \n",
    "    # \u2500\u2500 Dark spectra class distribution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(\"Dark spectra per polymer:\")\n",
    "    from collections import Counter\n",
    "    dark_counts = Counter(y_dark)\n",
    "    for polymer, count in sorted(dark_counts.items()):\n",
    "        print(f\"  {polymer}: {count}\")\n",
    "    print()\n",
    "    \n",
    "    # \u2500\u2500 Stage 1: Light vs Dark binary classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STAGE 1: Light vs Dark Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_binary = np.vstack([X_light, X_dark])\n",
    "    y_binary = np.array(['light'] * len(X_light) + ['dark'] * len(X_dark))\n",
    "    \n",
    "    for name, model in [(\"kNN (k=5)\", KNeighborsClassifier(n_neighbors=5)),\n",
    "                        (\"SVM (RBF)\", SVC(kernel='rbf', gamma='scale'))]:\n",
    "        scores = cross_val_score(model, X_binary, y_binary, cv=5, scoring='accuracy')\n",
    "        print(f\"  {name}: {scores.mean():.3f} \u00b1 {scores.std():.3f} (5-fold CV accuracy)\")\n",
    "    print()\n",
    "    \n",
    "    # \u2500\u2500 Stage 2b: Dark polymer classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STAGE 2b: Polymer Classification on Dark Spectra Only\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Only include polymer types with at least 3 samples (minimum for CV)\n",
    "    valid_polymers = [p for p, c in dark_counts.items() if c >= 3]\n",
    "    mask = np.isin(y_dark, valid_polymers)\n",
    "    X_dark_valid = X_dark[mask]\n",
    "    y_dark_valid = y_dark[mask]\n",
    "    \n",
    "    excluded = [p for p, c in dark_counts.items() if c < 3]\n",
    "    if excluded:\n",
    "        print(f\"  Excluded (< 3 samples): {excluded}\")\n",
    "    print(f\"  Using {len(X_dark_valid)} spectra across {len(valid_polymers)} polymer types\")\n",
    "    print()\n",
    "    \n",
    "    if len(valid_polymers) < 2:\n",
    "        print(\"  Not enough polymer types with sufficient samples for classification.\")\n",
    "        return\n",
    "    \n",
    "    # Use leave-one-out or small CV for limited data\n",
    "    cv_folds = min(3, min(dark_counts[p] for p in valid_polymers))\n",
    "    \n",
    "    for name, model in [(\"kNN (k=3)\", KNeighborsClassifier(n_neighbors=min(3, len(X_dark_valid) - 1))),\n",
    "                        (\"SVM (RBF)\", SVC(kernel='rbf', gamma='scale'))]:\n",
    "        scores = cross_val_score(model, X_dark_valid, y_dark_valid, cv=cv_folds, scoring='accuracy')\n",
    "        print(f\"  {name}: {scores.mean():.3f} \u00b1 {scores.std():.3f} ({cv_folds}-fold CV accuracy)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Dark classification results should be interpreted cautiously\")\n",
    "    print(\"due to small sample sizes per polymer group.\")\n",
    "\n",
    "\n",
    "# \u2500\u2500 Run the extension \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "dark_classifier_experiment(checked_path, outlier_checked_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the two-stage results:**\n",
    "\n",
    "- **Stage 1 (light vs dark):** If accuracy is high (>95%), this confirms that light and dark spectra are systematically different and a two-stage pipeline is feasible. The physical differences (carbon black absorption, suppressed features) make this an easy binary task.\n",
    "\n",
    "- **Stage 2b (dark polymer classification):** This is the critical test. If accuracy is substantially above chance level (1/N where N = number of polymer types), it means some polymer-specific information survives despite the pigment interference. Even moderate accuracy (40\u201360%) would suggest that a dedicated dark-plastic model \u2014 potentially with more training data \u2014 could be viable.\n",
    "\n",
    "- **If dark classification is poor:** This confirms that carbon black absorption is too dominant for the 1550\u20131950 nm range, and dark plastic identification would require either a wider spectral range, a different technique (e.g., mid-IR, Raman), or supplementary features (e.g., density, melting point).\n",
    "\n",
    "This extension demonstrates the value of the colour-based separation approach: by understanding the physical causes of spectral variation, we can design targeted solutions rather than treating all anomalies as noise to be discarded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}