{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID1: Section B — Building an AI Model for Polymer Identification\n",
    "\n",
    "**Assigned Polymer:** PET (Polyethylene terephthalate)\n",
    "\n",
    "This notebook implements the data exploration, visualisation, and machine learning tasks required for Section B of the ID1 lab. NIR spectral data collected using the PlasTell spectrophotometer is processed, cleaned, and used to train k-Nearest Neighbours (kNN) classifiers for polymer identification.\n",
    "\n",
    "**Data sources:**\n",
    "- `synthetic_data.csv` — Synthetic PlasTell export for code development (NOT used in final ML models)\n",
    "- `data_source2.csv` — Lab-provided reference spectra to combine with own data in Task 2.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Imports and Configuration\n\nBefore any analysis, we need to load the Python **libraries** (pre-written code packages) that provide the tools we'll use throughout:\n\n| Library | What it does | Why we need it |\n|---|---|---|\n| `pandas` | Handles tabular data (like spreadsheets) | Loading CSV files, filtering columns, counting values |\n| `numpy` | Fast maths on arrays of numbers | Calculating means, standard deviations, manipulating spectral arrays |\n| `matplotlib` / `seaborn` | Plotting graphs and heatmaps | Visualising spectra, confusion matrices, PCA scatter plots |\n| `ast` | Safely converts text strings to Python objects | The PlasTell CSV stores spectra as text like `\"[54406, 54428, ...]\"` — we need to convert these to actual number lists |\n| `hashlib` | Generates unique \"fingerprints\" (hashes) for data | Used to detect duplicate spectra — two identical spectra will produce the same hash |\n| `sklearn` | Machine learning toolkit | Train/test splitting, scaling, kNN classifiers, evaluation metrics |\n\nWe also set a **random seed** and define **file paths** — both explained in the comments below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Data handling libraries ───────────────────────────────────────────────────\nimport pandas as pd          # For loading/manipulating CSV data as tables (\"DataFrames\")\nimport numpy as np           # For numerical operations on arrays (mean, std, etc.)\n\n# ── Plotting libraries ───────────────────────────────────────────────────────\nimport matplotlib.pyplot as plt  # Core plotting library (line plots, scatter, bar charts)\nimport seaborn as sns            # Higher-level plotting built on matplotlib (heatmaps, styled plots)\n\n# ── Utility libraries ────────────────────────────────────────────────────────\nimport ast                   # ast.literal_eval() safely converts a string like \"[1, 2, 3]\" into an actual Python list\nimport hashlib               # Generates SHA-256 hashes — a \"digital fingerprint\" to detect duplicate spectra\nimport csv                   # (Not actually used, but available for CSV manipulation if needed)\nimport os                    # For building file paths that work on any operating system\n\n# ── Data structures ──────────────────────────────────────────────────────────\nfrom collections import Counter  # Quickly counts how many times each item appears in a list\n\n# ── Machine learning (scikit-learn) ──────────────────────────────────────────\nfrom sklearn.model_selection import train_test_split   # Splits data into training set and test set\nfrom sklearn.preprocessing import StandardScaler       # Centres and scales features to mean=0, std=1\nfrom sklearn.neighbors import KNeighborsClassifier     # The kNN classification algorithm\nfrom sklearn.metrics import confusion_matrix, classification_report  # Evaluate how well the model performed\n\n# ── Reproducibility ──────────────────────────────────────────────────────────\n# Many ML operations involve randomness (e.g., which spectra go into the\n# training vs test set). Setting a \"seed\" fixes the random number generator\n# so that every time you run this notebook, you get EXACTLY the same split,\n# the same results, and the same plots. The number 42 is arbitrary — any\n# integer would work — but using the same seed means your results are\n# reproducible by anyone who runs this code. This is essential for scientific\n# work: an examiner re-running your notebook should see identical output.\nnp.random.seed(42)\n\n# ── File paths ───────────────────────────────────────────────────────────────\n# os.path.join() builds file paths using the correct separator for your OS\n# (/ on Mac/Linux, \\ on Windows). The '.' means \"current directory\" — i.e.,\n# the folder this notebook is saved in.\nDATA_DIR = os.path.join('.', 'data')       # Where the raw input CSV files live\nOUTPUT_DIR = os.path.join('.', 'output')   # Where we'll save processed/cleaned files\n\n# Input file paths — these point to the two data sources described in cell 0\nPLASTELL_CSV = os.path.join(DATA_DIR, 'synthetic_data.csv')      # Your PlasTell spectrophotometer export\nDATA_SOURCE2_CSV = os.path.join(DATA_DIR, 'data_source2.csv')    # Lab-provided reference spectra\n\nprint('Setup complete.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 2: Exploring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.01: Counting Spectra & Polymers from your .CSV File\n\n**Objective:** Load the NIR spectral data from the PlasTell .CSV export, count the total number of spectra, and determine how many times each unique polymer appears in the dataset.\n\n**Why this matters:** Before doing any analysis or machine learning, you need to know:\n- **How much data you have** — too little data makes ML models unreliable.\n- **How the data is distributed across classes** — if one polymer has 100 spectra and another has 3, the model will be biased towards the larger class (this is called **class imbalance**)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_spectra(filepath):\n    \"\"\"Load a CSV file and count how many spectra (rows) it contains.\n    \n    Each row in the PlasTell export represents one spectrum measurement\n    taken from one polymer sample.\n    \"\"\"\n    df = pd.read_csv(filepath)   # pd.read_csv() reads a CSV file into a DataFrame (table)\n    total = len(df)              # len() counts the number of rows = number of spectra\n    print(f'Total number of spectra: {total}.')\n    return df                    # Return the DataFrame so we can use it in later tasks\n\n\ndef count_polymers(df):\n    \"\"\"Count how many spectra exist for each unique polymer type.\n    \n    The 'labelMaterialsString' column contains the polymer name (e.g., 'PET',\n    'HDPE') for each spectrum. value_counts() tallies how many times each\n    unique name appears — like doing a frequency count in a spreadsheet.\n    \"\"\"\n    counts = df['labelMaterialsString'].value_counts()\n    print(f'\\nNumber of spectra for each unique polymer:')\n    print(counts)\n    return counts\n\n\n# ── Run Task 2.01 ────────────────────────────────────────────────────────────\n# Load the raw PlasTell data and count what we have\nraw_df = count_spectra(PLASTELL_CSV)\npolymer_counts = count_polymers(raw_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The dataset contains spectra for several polymer types. The distribution shows whether the dataset is **balanced** (roughly equal numbers of each polymer) or **imbalanced** (some polymers hugely over- or under-represented).\n\n**Why balance matters:** If PMMA has 100 spectra but ABS only has 5, a lazy model could predict \"PMMA\" for everything and still get ~95% accuracy — without actually learning to identify ABS. This is why we check the distribution early and address it later (see the Class Imbalance section)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.02: Filtering and Saving Specific Data\n\n**Objective:** Extract only the two columns we actually need — the polymer label and the spectrum data — and save them to a new, cleaner CSV file.\n\n**Why we do this:** The raw PlasTell export contains many columns of metadata (timestamps, device info, scan settings, etc.) that are irrelevant for ML. Keeping only `labelMaterialsString` (the polymer name) and `spectrum` (the intensity values) removes clutter and reduces file size. Think of it like highlighting only the relevant columns in a spreadsheet and copying them to a new sheet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def filter_and_save(df, output_path):\n    \"\"\"Keep only the polymer label and spectrum columns; discard everything else.\n    \n    .copy() creates an independent copy of the data so that changes to the\n    filtered version don't accidentally modify the original DataFrame.\n    \"\"\"\n    filtered = df[['labelMaterialsString', 'spectrum']].copy()  # Select just these 2 columns\n    filtered.to_csv(output_path, index=False)                   # Save to CSV (index=False avoids adding a row-number column)\n    print(f'Filtered data saved to: {output_path}')\n    print(f'\\nFirst 5 rows:')\n    print(filtered.head())    # .head() shows the first 5 rows — a quick sanity check\n    return filtered\n\n\n# ── Run Task 2.02 ────────────────────────────────────────────────────────────\nfiltered_path = os.path.join(OUTPUT_DIR, '2.02_filtered-data.csv')\nfiltered_df = filter_and_save(raw_df, filtered_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.03: Transforming Spectrophotometer Data\n\n**Objective:** Convert the raw data into a format suitable for analysis and ML.\n\n**The problem:** In the PlasTell CSV, each spectrum is stored as a **text string** that looks like a Python list: `\"[54406, 54428, 54691, ...]\"`. Python can't do maths on text — we need to convert these strings into actual arrays of numbers.\n\n**What this function does, step by step:**\n1. Uses `ast.literal_eval()` to safely convert each text string into a real Python list of integers.\n2. Calculates the **wavelength axis** — the PlasTell scans from 1550 nm to 1950 nm across 128 evenly-spaced points. `np.linspace(1550, 1950, 128)` generates these 128 wavelength values.\n3. **Transposes** the data so that each row = one wavelength and each column = one spectrum. This is the standard format for spectral databases (wavelengths on the y-axis, samples along the x-axis).\n4. Names each column by its polymer type (e.g., `HDPE`, `HDPE.01`, `HDPE.02`, ...) so we can identify which spectrum belongs to which polymer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_spectral_data(input_path, output_path):\n    \"\"\"Convert string-encoded spectra into a numeric wavelength x spectra table.\n    \n    Input format:  Each row has a polymer label and a spectrum stored as text.\n    Output format: Each row is a wavelength (1550-1950 nm), each column is one spectrum.\n    \"\"\"\n    df = pd.read_csv(input_path)\n    \n    # Step 1: Convert spectrum strings to actual Python lists of numbers\n    # e.g., the string \"[54406, 54428, ...]\" becomes the list [54406, 54428, ...]\n    df['spectrum'] = df['spectrum'].apply(ast.literal_eval)\n    \n    # Step 2: Calculate the wavelength axis\n    # The PlasTell measures 128 intensity values evenly spaced from 1550 nm to 1950 nm\n    num_points = len(df['spectrum'].iloc[0])           # How many points per spectrum (should be 128)\n    wavelengths = np.linspace(1550, 1950, num_points)  # Generate 128 evenly-spaced wavelengths\n    \n    # Step 3: Build unique column names for each spectrum\n    # First HDPE spectrum → \"HDPE\", second → \"HDPE.01\", third → \"HDPE.02\", etc.\n    labels = df['labelMaterialsString'].values\n    polymer_counter = {}   # Tracks how many spectra we've seen for each polymer\n    column_names = []\n    for label in labels:\n        if label not in polymer_counter:\n            polymer_counter[label] = 0\n        polymer_counter[label] += 1\n        count = polymer_counter[label]\n        if count == 1:\n            column_names.append(label)                      # First spectrum: just \"HDPE\"\n        else:\n            column_names.append(f'{label}.{count - 1:02d}')  # Subsequent: \"HDPE.01\", \"HDPE.02\", etc.\n    \n    # Step 4: Transpose so rows = wavelengths, columns = spectra\n    # .tolist() converts each spectrum list into rows of a 2D array\n    # .T transposes from (n_spectra, 128) to (128, n_spectra)\n    spectra_array = np.array(df['spectrum'].tolist()).T\n    result = pd.DataFrame(spectra_array, columns=column_names)\n    result.insert(0, 'Wavelength (nm)', wavelengths)  # Add wavelength as the first column\n    \n    result.to_csv(output_path, index=False)\n    print(f'Transformed data saved to: {output_path}')\n    print(f'Shape: {result.shape[0]} wavelengths x {result.shape[1] - 1} spectra')\n    print(f'\\nFirst 5 rows:')\n    print(result.head())\n    return result\n\n\n# ── Run Task 2.03 ────────────────────────────────────────────────────────────\ntransformed_path = os.path.join(OUTPUT_DIR, '2.03_transformed-data.csv')\ntransformed_df = transform_spectral_data(filtered_path, transformed_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in the required transposed format: each row corresponds to a wavelength value (1550–1950 nm) and each column holds a single spectrum labelled by polymer type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.04: Combining Datasets\n\n**Objective:** Merge your own PlasTell spectra with the lab-provided reference dataset (`data_source2.csv`) into one large combined database.\n\n**Why we do this:** Your own PlasTell data only has ~35 spectra (3-5 per polymer). That's far too few for reliable ML. The lab-provided dataset adds ~335 more spectra, giving the model more examples to learn from. More data = better generalisation = more reliable predictions.\n\n**How it works:** Both datasets have the same structure — 128 wavelength points from 1550-1950 nm — so we can simply stick them side-by-side (column-wise concatenation). We drop the duplicate wavelength column from the second dataset to avoid having it twice."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def combine_datasets(path1, path2, output_path):\n    \"\"\"Combine two spectral datasets side-by-side (column-wise).\n    \n    Both datasets must have the same number of rows (128 wavelengths).\n    The wavelength column from dataset 2 is dropped to avoid duplication.\n    \"\"\"\n    df1 = pd.read_csv(path1)   # Our transformed PlasTell data\n    df2 = pd.read_csv(path2)   # The lab-provided reference spectra\n    \n    # Drop the wavelength column from dataset 2 — we already have it from dataset 1\n    df2_spectra = df2.iloc[:, 1:]   # iloc[:, 1:] means \"all rows, columns from index 1 onwards\" (skipping column 0)\n    \n    # pd.concat with axis=1 joins tables side-by-side (adding columns)\n    # axis=0 would stack them vertically (adding rows)\n    combined = pd.concat([df1, df2_spectra], axis=1)\n    combined.to_csv(output_path, index=False)\n    \n    print(f'Combined data saved to: {output_path}')\n    print(f'Dataset 1: {df1.shape[1] - 1} spectra')       # -1 to exclude the wavelength column\n    print(f'Dataset 2: {df2_spectra.shape[1]} spectra')\n    print(f'Combined: {combined.shape[1] - 1} total spectra')\n    return combined\n\n\n# ── Run Task 2.04 ────────────────────────────────────────────────────────────\ncombined_path = os.path.join(OUTPUT_DIR, '2.04_combined-data.csv')\ncombined_df = combine_datasets(transformed_path, DATA_SOURCE2_CSV, combined_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.05: Counting Columns by Polymer\n\n**Objective:** Now that we've combined both datasets, count how many spectra we have for each polymer type.\n\n**How column names encode the polymer:** Each column is named like `POLYMER` or `POLYMER.XX` (e.g., `PMMA`, `PMMA.01`, `PMMA.02`). By splitting on the `.` character and taking the first part, we extract the polymer name. Then `Counter` tallies up the totals — this is our \"class distribution\" for ML."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def count_columns_by_polymer(filepath):\n    \"\"\"Count how many spectra columns exist for each polymer type.\n    \n    Column names like 'PMMA.05' are split on '.' to extract just 'PMMA'.\n    Counter then tallies how many columns share each polymer prefix.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    headers = df.columns[1:]  # Skip the first column (wavelength) — everything else is a spectrum\n    \n    # Extract polymer name from each column header\n    # 'PMMA.05'.split('.')[0] → 'PMMA'\n    # 'PET'.split('.')[0] → 'PET' (still works even without a dot)\n    prefixes = [h.split('.')[0] for h in headers]\n    counts = Counter(prefixes)  # Counter({'PMMA': 118, 'PVC': 73, ...})\n    \n    print(f'Total number of spectra columns: {len(headers)}')\n    print(f'\\nSpectra count per polymer type:')\n    for polymer, count in sorted(counts.items()):\n        print(f'  {polymer}: {count}')\n    return counts\n\n\n# ── Run Task 2.05 ────────────────────────────────────────────────────────────\npolymer_column_counts = count_columns_by_polymer(combined_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced polymer representation can bias the kNN model towards over-represented classes. If significant imbalances exist, strategies such as undersampling, oversampling, or stratified splitting should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.06: Looking for Duplicates\n\n**Objective:** Detect and remove spectra that are exact copies of each other.\n\n**Why duplicates are a problem:** When we combined datasets (Task 2.04), some spectra from our PlasTell data may already exist in the lab-provided dataset — they'd be counted twice. Duplicates artificially inflate the dataset and bias the ML model by over-representing those particular measurements.\n\n**How we detect them — SHA-256 hashing:**\n- A **hash function** takes any data and produces a fixed-length \"fingerprint\" string (e.g., `a3f5b2c1...`).\n- If two spectra contain **exactly the same** 128 intensity values, they produce **the same hash**.\n- If even a single value differs, the hashes will be completely different.\n- This lets us efficiently compare hundreds of spectra without checking every value pair-by-pair.\n\nWe keep the first copy and remove all subsequent duplicates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_hash(spectrum_values):\n    \"\"\"Generate a unique SHA-256 fingerprint for a single spectrum.\n    \n    Converts the 128 intensity values to a string, then feeds it through\n    the SHA-256 algorithm to produce a 64-character hex digest. Two spectra\n    with identical values will always produce the same hash.\n    \"\"\"\n    data_string = str(spectrum_values.tolist())                     # Convert array to string: \"[54406, 54428, ...]\"\n    return hashlib.sha256(data_string.encode('utf-8')).hexdigest()  # Hash it → 64-char hex string\n\n\ndef find_and_remove_duplicates(filepath, output_path):\n    \"\"\"Find spectra that are exact copies and remove the duplicates.\n    \n    Strategy: Hash each spectrum column. If two columns share the same hash,\n    they contain identical data. Keep the first occurrence, drop the rest.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    spectra_columns = df.columns[1:]  # Everything except the wavelength column\n    \n    # Build a dictionary: hash → list of column names that share that hash\n    hash_dict = {}\n    for col in spectra_columns:\n        h = compute_hash(df[col])\n        if h not in hash_dict:\n            hash_dict[h] = []\n        hash_dict[h].append(col)\n    \n    # Any hash with more than one column = those columns are duplicates\n    duplicates = {h: cols for h, cols in hash_dict.items() if len(cols) > 1}\n    \n    if duplicates:\n        print('Duplicate spectra found:')\n        columns_to_drop = []\n        for h, cols in duplicates.items():\n            print(f'  Columns with identical spectra: {cols}')\n            columns_to_drop.extend(cols[1:])   # Keep the first, mark the rest for removal\n        \n        df_cleaned = df.drop(columns=columns_to_drop)  # Remove duplicate columns\n        print(f'\\nRemoved {len(columns_to_drop)} duplicate column(s).')\n    else:\n        print('No duplicate spectra found.')\n        df_cleaned = df\n    \n    df_cleaned.to_csv(output_path, index=False)\n    print(f'Duplicate-checked data saved to: {output_path}')\n    print(f'Remaining spectra: {df_cleaned.shape[1] - 1}')\n    return df_cleaned\n\n\n# ── Run Task 2.06 ────────────────────────────────────────────────────────────\ndup_checked_path = os.path.join(OUTPUT_DIR, '2.06_duplicate-checked-data.csv')\ndup_checked_df = find_and_remove_duplicates(combined_path, dup_checked_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 2.07: Checking for Invalid Values in Transformed Data\n\n**Objective:** Scan every data point in the dataset for **missing values** (NaN) and **non-numeric entries** (e.g., the word `\"none\"` appearing where a number should be).\n\n**Why this matters:** ML algorithms require clean numerical input. If even one cell contains text like `\"none\"` instead of a number, the model will crash or produce nonsensical results. The lab-provided `data_source2.csv` is known to contain some `\"none\"` entries.\n\n**How we fix invalid values — median imputation:**\n- We replace each invalid value with the **median** of its column (i.e., the middle value when all valid intensity readings at that wavelength are sorted).\n- **Why median, not mean?** The median is robust to outliers. If one reading is abnormally high (e.g., a sensor glitch), the mean would be pulled toward that extreme value, but the median would barely change. This preserves the typical spectral shape more faithfully."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_and_fix_invalid_values(filepath, output_path):\n    \"\"\"Scan for missing/non-numeric values and replace them with the column median.\n    \n    Two types of problems are checked:\n    1. NaN (Not a Number) — empty cells or values that pandas couldn't parse\n    2. Non-numeric strings — e.g., \"none\" appearing where a number should be\n    \n    Both are replaced with the column's median value (median imputation).\n    \"\"\"\n    df = pd.read_csv(filepath)\n    issues_found = False\n    \n    for col in df.columns[1:]:  # Skip the wavelength column — only check spectrum columns\n        # Check 1: Are there any NaN (missing) values?\n        nan_mask = df[col].isna()          # Creates True/False for each cell: True = missing\n        if nan_mask.any():                  # .any() returns True if at least one cell is NaN\n            issues_found = True\n            nan_rows = df.index[nan_mask].tolist()\n            print(f'  Missing value (NaN) in column \"{col}\" at row(s): {[r + 2 for r in nan_rows]}')\n        \n        # Check 2: Are there any values that look like numbers but are actually text?\n        for idx, value in df[col].items():\n            if pd.isna(value):\n                continue                    # Already flagged above, skip it\n            try:\n                float(value)                # Try converting to a number\n            except (ValueError, TypeError):  # If this fails, it's not a valid number\n                issues_found = True\n                print(f'  Non-numeric value \"{value}\" in column \"{col}\" at row {idx + 2}')\n                df.at[idx, col] = np.nan    # Mark it as NaN so we can impute it below\n    \n    if not issues_found:\n        print('No invalid values found.')\n    else:\n        # Replace all NaN values with the column median\n        print('\\nImputing invalid values with column median...')\n        for col in df.columns[1:]:\n            df[col] = pd.to_numeric(df[col], errors='coerce')  # Force everything to numbers (non-numeric → NaN)\n            if df[col].isna().any():\n                median_val = df[col].median()                   # Calculate the median of valid values\n                df[col] = df[col].fillna(median_val)            # Replace NaN with median\n                print(f'  Column \"{col}\": filled with median = {median_val:.1f}')\n    \n    df.to_csv(output_path, index=False)\n    print(f'\\nChecked data saved to: {output_path}')\n    return df\n\n\n# ── Run Task 2.07 ────────────────────────────────────────────────────────────\nchecked_path = os.path.join(OUTPUT_DIR, '2.07_checked-data.csv')\nchecked_df = check_and_fix_invalid_values(dup_checked_path, checked_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median imputation was chosen because it is robust to outliers — unlike the mean, a single extreme value will not distort the imputed result. Alternatively, the entire column could be discarded if too many values are invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 3: Visualising Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 3.01: Visualisation of Central Tendency & Dispersion\n\n**Objective:** For each polymer type, calculate the **mean spectrum** (average shape) and **standard deviation** (how much individual spectra vary from the average), then plot both.\n\n**Why this matters:**\n- The **mean spectrum** shows the \"typical\" NIR signature for each polymer — this is what the ML model is essentially trying to learn for each class.\n- The **standard deviation** (shown as a shaded band around the mean) shows how much **variation** exists within each polymer class. Large variation = harder for the model to learn a clear pattern.\n- If two polymers' mean spectra look very similar, the model will struggle to tell them apart.\n\n**Why we use the mean (not the median) here:** For continuous spectral intensity data that is roughly symmetrically distributed at each wavelength, the mean is a good summary. (We used the median earlier for *imputation* because it's more robust to extreme outliers — different use case.)\n\n**Why the x-axis is reversed:** In spectroscopy, it's convention to plot with higher wavenumbers (shorter wavelengths) on the left. Since we're plotting wavelength in nm, reversing the axis follows this convention."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_prefix(column_name):\n    \"\"\"Extract the polymer name from a column header.\n    \n    Examples:\n        'PET'    → 'PET'     (no dot, returns the whole name)\n        'PET.03' → 'PET'     (splits on '.', returns the first part)\n        'PMMA.105' → 'PMMA'\n    \n    This is used throughout the notebook whenever we need to group\n    spectra by their polymer type.\n    \"\"\"\n    return column_name.split('.')[0]\n\n\ndef plot_average_spectra(filepath, output_csv_path):\n    \"\"\"Calculate and plot the mean spectrum +/- standard deviation for each polymer.\n    \n    For each polymer group:\n    1. Collect all spectra belonging to that polymer\n    2. Calculate the mean intensity at each wavelength (the average shape)\n    3. Calculate the standard deviation at each wavelength (the spread)\n    4. Plot the mean as a line and the +/- 1 std dev as a shaded band\n    \"\"\"\n    df = pd.read_csv(filepath)\n    wavelengths = df.iloc[:, 0].values   # First column = wavelength values (1550-1950 nm)\n    spectra_columns = df.columns[1:]     # All other columns = individual spectra\n    \n    # Group columns by polymer prefix (e.g., all PMMA columns together)\n    groups = {}\n    for col in spectra_columns:\n        prefix = extract_prefix(col)\n        if prefix not in groups:\n            groups[prefix] = []\n        groups[prefix].append(col)\n    \n    # Calculate mean and std for each polymer group, and plot\n    results = {'Wavelength (nm)': wavelengths}\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for polymer in sorted(groups.keys()):\n        cols = groups[polymer]\n        # Convert to numeric (in case any strings remain), shape: (128 wavelengths, n_spectra)\n        data = df[cols].apply(pd.to_numeric, errors='coerce').values\n        mean_spectrum = np.nanmean(data, axis=1)  # Average across spectra at each wavelength\n        std_spectrum = np.nanstd(data, axis=1)    # Std dev across spectra at each wavelength\n        \n        results[f'{polymer}_mean'] = mean_spectrum\n        results[f'{polymer}_std'] = std_spectrum\n        \n        # Plot the mean line\n        ax.plot(wavelengths, mean_spectrum, label=polymer)\n        # Plot the shaded band showing +/- 1 standard deviation\n        ax.fill_between(wavelengths,\n                        mean_spectrum - std_spectrum,    # Lower bound\n                        mean_spectrum + std_spectrum,    # Upper bound\n                        alpha=0.2)                       # alpha=0.2 makes it semi-transparent\n    \n    ax.set_xlabel('Wavelength (nm)')\n    ax.set_ylabel('Intensity (a.u.)')         # a.u. = arbitrary units (PlasTell raw counts)\n    ax.set_title('Average Spectra with Standard Deviation')\n    ax.legend(loc='best', fontsize=8)\n    ax.invert_xaxis()                          # Spectroscopy convention: high wavenumber on left\n    plt.tight_layout()\n    plt.show()\n    \n    # Save the averaged data to CSV for reference\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(output_csv_path, index=False)\n    print(f'Averaged data saved to: {output_csv_path}')\n    return results_df\n\n\n# ── Run Task 3.01 ────────────────────────────────────────────────────────────\naveraged_path = os.path.join(OUTPUT_DIR, '3.01_averaged-data.csv')\naveraged_df = plot_average_spectra(checked_path, averaged_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that different polymers have distinct spectral signatures, particularly in the 1650–1850 nm region. Large error bars may indicate measurement variability or outlier spectra within that polymer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 3.02: Using Heatmaps to Explore Spectra Variation\n\n**Objective:** Generate per-polymer heatmaps of min-max normalised spectra and programmatically identify outlier spectra for removal. Each row is an individual spectrum and each column is a wavelength. Normalisation ensures all spectra are on the same 0–1 scale for fair comparison.\n\n**Outlier detection method — Pearson correlation to group mean:**\n\nRather than relying solely on visual inspection of heatmaps, outliers are detected quantitatively using the following approach:\n\n1. For each polymer group, the **mean spectrum** is calculated across all spectra in that group.\n2. The **Pearson correlation coefficient** is computed between each individual spectrum and its group mean. This measures how well the shape of each spectrum matches the typical shape for that polymer — a value of +1.0 means a perfect match, 0 means no relationship, and negative values indicate an inverted spectral pattern.\n3. Within each polymer group, a **z-score** is calculated for each correlation value. Spectra with z-scores below -2 (i.e., more than 2 standard deviations below the group average correlation) are flagged as statistical outliers.\n4. Any spectrum with a **negative correlation** to its group mean is also flagged, as this indicates an inverted or fundamentally different spectral shape that is physically inconsistent with the polymer label.\n5. As a safeguard, polymer groups where **all spectra have correlations above 0.999** are excluded from flagging, since any z-score outliers in such groups are statistical artifacts rather than genuine anomalies.\n\nThis approach is more objective and reproducible than purely visual inspection. Outlier rows are highlighted in red on the heatmaps for visual confirmation, and the flagged spectra are automatically removed before saving the cleaned dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from scipy.stats import pearsonr\n\n\ndef detect_outliers_correlation(data, cols, z_threshold=-2.0):\n    \"\"\"Detect outlier spectra using Pearson correlation to group mean.\n    \n    Each spectrum is correlated with the group mean spectrum. Spectra with\n    z-scores below the threshold or with negative correlations are flagged\n    as outliers. Groups where all correlations exceed 0.999 are skipped\n    (flags in such groups are statistical artifacts, not real outliers).\n    \n    Args:\n        data: array of shape (n_spectra, n_wavelengths), raw intensity values.\n        cols: list of column names corresponding to each spectrum.\n        z_threshold: z-score cutoff for flagging (default -2.0).\n    \n    Returns:\n        outlier_names: list of column names identified as outliers.\n        correlations: dict mapping column name to its Pearson correlation.\n    \"\"\"\n    mean_spectrum = np.nanmean(data, axis=0)\n    \n    # Compute Pearson correlation of each spectrum to the group mean\n    correlations = {}\n    for i, col in enumerate(cols):\n        corr, _ = pearsonr(data[i], mean_spectrum)\n        correlations[col] = corr\n    \n    corr_values = np.array(list(correlations.values()))\n    \n    # Skip groups where all correlations are extremely high (no real outliers)\n    if np.all(corr_values > 0.999):\n        return [], correlations\n    \n    # Z-score within the group\n    mean_corr = np.mean(corr_values)\n    std_corr = np.std(corr_values)\n    \n    outlier_names = []\n    if std_corr > 0:\n        for col, corr in correlations.items():\n            z = (corr - mean_corr) / std_corr\n            # Flag if z-score below threshold OR negative correlation\n            if z < z_threshold or corr < 0:\n                outlier_names.append(col)\n    else:\n        # Zero std means all spectra are identical — no outliers\n        pass\n    \n    return outlier_names, correlations\n\n\ndef plot_heatmaps(filepath, output_path):\n    \"\"\"Create min-max normalised heatmaps with automated outlier detection.\n    \n    For each polymer group, outliers are detected using Pearson correlation\n    to the group mean spectrum. Outlier rows are highlighted in red on the\n    heatmap y-axis labels. Detected outlier columns are removed from the\n    dataset before saving.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    wavelengths = df.iloc[:, 0].values\n    spectra_columns = df.columns[1:]\n    \n    # Group by polymer prefix\n    groups = {}\n    for col in spectra_columns:\n        prefix = extract_prefix(col)\n        if prefix not in groups:\n            groups[prefix] = []\n        groups[prefix].append(col)\n    \n    outlier_columns = []  # Automatically populated by detection algorithm\n    all_correlations = {}  # Store correlations for reporting\n    \n    for polymer in sorted(groups.keys()):\n        cols = groups[polymer]\n        data = df[cols].apply(pd.to_numeric, errors='coerce').values.T  # shape: (n_spectra, 128)\n        \n        # Detect outliers using correlation method\n        outliers, correlations = detect_outliers_correlation(data, cols)\n        outlier_columns.extend(outliers)\n        all_correlations.update(correlations)\n        \n        # Min-max normalisation per spectrum (row)\n        normalised = np.zeros_like(data, dtype=float)\n        for i in range(data.shape[0]):\n            row = data[i]\n            row_min, row_max = np.nanmin(row), np.nanmax(row)\n            if row_max - row_min > 0:\n                normalised[i] = (row - row_min) / (row_max - row_min)\n            else:\n                normalised[i] = 0  # Avoid division by zero\n        \n        # Create heatmap\n        fig, ax = plt.subplots(figsize=(14, max(3, len(cols) * 0.3)))\n        \n        # Round wavelengths for display\n        wl_labels = [f'{w:.0f}' for w in wavelengths]\n        heatmap_df = pd.DataFrame(normalised, index=cols, columns=wl_labels)\n        \n        # Show every Nth wavelength label for readability\n        n_labels = 10\n        step = max(1, len(wl_labels) // n_labels)\n        \n        sns.heatmap(heatmap_df, cmap='viridis', ax=ax,\n                    xticklabels=step, yticklabels=True)\n        ax.set_title(f'Heatmap of Normalised Spectra — {polymer}')\n        ax.set_xlabel('Wavelength (nm)')\n        ax.set_ylabel('Spectra Index')\n        ax.invert_xaxis()\n        \n        # Highlight outlier rows in red on the y-axis\n        outlier_set = set(outliers)\n        for tick_label in ax.get_yticklabels():\n            if tick_label.get_text() in outlier_set:\n                tick_label.set_color('red')\n                tick_label.set_fontweight('bold')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print per-polymer summary\n        if outliers:\n            print(f'  {polymer}: {len(outliers)} outlier(s) detected:')\n            for col in outliers:\n                print(f'    - {col} (correlation = {correlations[col]:.4f})')\n        else:\n            print(f'  {polymer}: no outliers detected.')\n    \n    # Print overall summary\n    print(f'\\n{\"=\"*60}')\n    print(f'OUTLIER DETECTION SUMMARY')\n    print(f'{\"=\"*60}')\n    print(f'Method: Pearson correlation to group mean (z < -2 or r < 0)')\n    print(f'Total outliers detected: {len(outlier_columns)}')\n    if outlier_columns:\n        # Group outliers by polymer for summary\n        outlier_by_polymer = {}\n        for col in outlier_columns:\n            prefix = extract_prefix(col)\n            if prefix not in outlier_by_polymer:\n                outlier_by_polymer[prefix] = []\n            outlier_by_polymer[prefix].append(col)\n        \n        for polymer in sorted(outlier_by_polymer.keys()):\n            cols_list = outlier_by_polymer[polymer]\n            print(f'  {polymer}: {len(cols_list)} — {cols_list}')\n    print(f'{\"=\"*60}\\n')\n    \n    # Remove outliers and save cleaned data\n    if outlier_columns:\n        df_cleaned = df.drop(columns=outlier_columns)\n        print(f'Removed {len(outlier_columns)} outlier column(s).')\n    else:\n        df_cleaned = df\n        print('No outliers detected.')\n    \n    df_cleaned.to_csv(output_path, index=False)\n    print(f'Outlier-checked data saved to: {output_path}')\n    print(f'Remaining spectra: {df_cleaned.shape[1] - 1}')\n    return df_cleaned\n\n\n# Run Task 3.02\noutlier_checked_path = os.path.join(OUTPUT_DIR, '3.02_outlier-checked-data.csv')\noutlier_checked_df = plot_heatmaps(checked_path, outlier_checked_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Interpreting the heatmaps and outlier detection results:**\n\nOutlier spectra are highlighted with **red labels** on the y-axis of each heatmap. These spectra were flagged because their spectral shape (as measured by Pearson correlation) deviates significantly from the group average — either by falling more than 2 standard deviations below the group's mean correlation, or by being negatively correlated with the group mean (indicating an inverted or fundamentally different spectral pattern).\n\nCommon causes of outlier spectra in NIR spectrophotometry include:\n- **Incorrect sample placement** on the sensor window\n- **Sample movement** during measurement\n- **Contamination** or mixed-material samples\n- **Sensor saturation** (values clipping at 0 or 65535)\n- **Different polymer grades** or additives that alter the spectral signature\n\nNote: Polymer groups where all spectra are near-identical (all correlations > 0.999) are excluded from z-score flagging, as minor statistical variations in such groups do not represent genuine anomalies.\n\nThe flagged outliers have been automatically removed and the cleaned dataset saved for use in subsequent classification tasks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 4: Classifying Your Data using a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.01: Spectral Data Classification Using k-Nearest Neighbours (kNN)\n\n**Objective:** Train a kNN classifier to identify polymers from their NIR spectra.\n\n**How kNN works (plain English):**\n1. You have a new unknown spectrum and want to know which polymer it is.\n2. kNN looks at the `k` most similar spectra in the training set (the \"nearest neighbours\").\n3. Whatever polymer type appears most among those `k` neighbours becomes the prediction.\n4. \"Similarity\" is measured by **Euclidean distance** — essentially, how close two spectra are when treated as points in 128-dimensional space (one dimension per wavelength).\n\n**Key steps in the pipeline:**\n\n| Step | What it does | Why |\n|---|---|---|\n| `train_test_split(test_size=0.5)` | Randomly splits data 50/50 into training and test sets | We train on one half and evaluate on the other — this tests whether the model generalises to unseen data |\n| `stratify=y` | Ensures the split keeps the same polymer proportions in both halves | Without this, the test set might randomly have no ABS samples |\n| `random_state=42` | Fixes the random split so results are reproducible | Same reason as `np.random.seed(42)` in the setup cell |\n| `StandardScaler` | Centres each feature to mean=0 and scales to std=1 | kNN uses distances — if one wavelength has values ~60000 and another ~100, the first would dominate the distance calculation unfairly. Scaling makes all wavelengths equally important |\n| `fit_transform` vs `transform` | `fit_transform` on training data learns the mean/std, then scales. `transform` on test data uses the *same* mean/std | Critical: the test set must be scaled using training statistics, not its own — otherwise you're \"peeking\" at test data (data leakage) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_spectral_data(filepath):\n    \"\"\"Load a spectral CSV and convert it into ML-ready format.\n    \n    Input:  CSV with wavelengths as rows, spectra as columns\n    Output: X = feature matrix (n_samples x 128 features)\n            y = label array (polymer name for each sample)\n            wavelengths = the 128 wavelength values\n    \n    The key transformation is the TRANSPOSE (.T): in the CSV, each column\n    is one spectrum (128 rows). But ML expects each ROW to be one sample.\n    So we flip the matrix: (128 wavelengths x n_spectra) → (n_spectra x 128 features).\n    \"\"\"\n    df = pd.read_csv(filepath)\n    wavelengths = df.iloc[:, 0].values       # The wavelength column (1550-1950 nm)\n    spectra_columns = df.columns[1:]         # Every other column is a spectrum\n    \n    # Transpose: each spectrum (column) becomes a row (sample)\n    # .apply(pd.to_numeric, errors='coerce') ensures everything is a number\n    X = df[spectra_columns].apply(pd.to_numeric, errors='coerce').values.T\n    \n    # Extract the polymer label from each column name\n    y = np.array([extract_prefix(col) for col in spectra_columns])\n    \n    return X, y, wavelengths\n\n\ndef run_knn_classification(filepath, k=5, test_size=0.5):\n    \"\"\"Train and evaluate a standard kNN classifier on spectral data.\n    \n    Pipeline: load data → split → scale → train kNN → predict → evaluate\n    \"\"\"\n    X, y, _ = prepare_spectral_data(filepath)\n    \n    # Split data: 50% for training, 50% for testing\n    # stratify=y ensures each polymer is proportionally represented in both sets\n    # random_state=42 makes the split reproducible (same split every time)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42, stratify=y\n    )\n    \n    # StandardScaler: centre each of the 128 wavelength features to mean=0, std=1\n    # This is ESSENTIAL for kNN because it uses Euclidean distance — without scaling,\n    # wavelengths with larger raw values would dominate the distance calculation\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)  # Learn mean/std from training data, then scale it\n    X_test_scaled = scaler.transform(X_test)         # Scale test data using the SAME mean/std (no peeking!)\n    \n    # Train the kNN model: k=5 means \"look at the 5 nearest neighbours\"\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)   # \"Fitting\" kNN just stores the training data — it's a lazy learner\n    \n    # Predict polymer labels for the test set\n    y_pred = knn.predict(X_test_scaled)\n    \n    # Print evaluation metrics\n    print(f'Occurrences of each polymer type in the test set:')\n    test_counts = pd.Series(y_test).value_counts()\n    print(test_counts)\n    \n    print(f'\\nClassification Report (k={k}):')\n    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n    \n    # Confusion matrix: rows = actual polymer, columns = predicted polymer\n    # Diagonal = correct predictions, off-diagonal = misclassifications\n    cm = confusion_matrix(y_test, y_pred, labels=labels)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('Predicted Label')\n    ax.set_ylabel('True Label')\n    ax.set_title(f'Confusion Matrix (kNN, k={k})')\n    plt.tight_layout()\n    plt.show()\n    \n    return knn, scaler\n\n\n# ── Run Task 4.01 ────────────────────────────────────────────────────────────\n# Train kNN with k=5 on the cleaned (post-outlier) dataset\nknn_model, knn_scaler = run_knn_classification(outlier_checked_path, k=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**How to read the classification report:**\n\n| Metric | What it means | Example |\n|---|---|---|\n| **Precision** | \"Of all the times the model said 'PET', what fraction was actually PET?\" | Precision = 0.80 means 80% of PET predictions were correct, 20% were wrong |\n| **Recall** | \"Of all the actual PET samples, what fraction did the model correctly identify?\" | Recall = 0.90 means the model found 90% of PET samples, missed 10% |\n| **F1-score** | The harmonic mean of precision and recall — a single number balancing both | High F1 = good at both finding the polymer AND not mislabelling others as it |\n| **Support** | How many test samples exist for that polymer | Low support (e.g., 2-3) means the metric is unreliable for that class |\n\n**How to read the confusion matrix:**\n- Each **row** = the actual (true) polymer type.\n- Each **column** = what the model predicted.\n- **Diagonal cells** (top-left to bottom-right) = correct predictions.\n- **Off-diagonal cells** = mistakes. For example, a \"3\" in row PVC / column PMMA means the model misidentified 3 PVC spectra as PMMA."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.02: Building a Weighted-kNN Model\n\n**Objective:** Improve on standard kNN with two enhancements: **distance weighting** and a **certainty threshold**.\n\n**Problem with standard kNN:** All `k` neighbours get an equal vote, regardless of how close or far they are. A very distant neighbour (which might belong to a different class) counts just as much as the closest neighbour.\n\n**Enhancement 1 — Distance weighting (`weights='distance'`):**\n- Closer neighbours get a **stronger vote** than distant ones.\n- Technically, each neighbour's vote is weighted by `1/distance` — so a neighbour at distance 0.1 has 10x more influence than one at distance 1.0.\n- This makes the model less sensitive to outliers lurking at the edges of clusters.\n\n**Enhancement 2 — Certainty threshold (0.6):**\n- After predicting, the model checks its **confidence** (the fraction of weighted votes for the winning class).\n- If confidence is below 60%, the prediction is labelled **\"Uncertain\"** instead of forcing a potentially wrong answer.\n- **Why this matters for polymer recycling:** Misidentifying a polymer contaminates the recycling stream. It's better to flag a sample as \"uncertain\" and re-test it than to confidently give the wrong answer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_weighted_knn(filepath, k=5, test_size=0.5, certainty_threshold=0.6):\n",
    "    \"\"\"Train and evaluate a distance-weighted kNN with a certainty threshold.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Weighted kNN: weights='distance' gives closer neighbours more influence\n",
    "    wknn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    wknn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict with probabilities\n",
    "    y_proba = wknn.predict_proba(X_test_scaled)\n",
    "    y_pred_raw = wknn.predict(X_test_scaled)\n",
    "    certainties = np.max(y_proba, axis=1)\n",
    "    \n",
    "    # Apply certainty threshold\n",
    "    y_pred = np.where(certainties >= certainty_threshold, y_pred_raw, 'Uncertain')\n",
    "    \n",
    "    # Counts\n",
    "    print('Occurrences of each polymer type in the test set:')\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    print(f'\\nPrediction counts (including Uncertain):')\n",
    "    print(pd.Series(y_pred).value_counts())\n",
    "    \n",
    "    # Classification report\n",
    "    all_labels = sorted(set(list(y_test) + list(y_pred)))\n",
    "    print(f'\\nClassification Report (Weighted kNN, k={k}, threshold={certainty_threshold}):')\n",
    "    print(classification_report(y_test, y_pred, labels=all_labels, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=all_labels)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=all_labels, yticklabels=all_labels, ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    axes[0].set_title(f'Confusion Matrix (Weighted kNN, k={k})')\n",
    "    \n",
    "    # Certainty histogram\n",
    "    axes[1].hist(certainties, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=certainty_threshold, color='red', linestyle='--',\n",
    "                    label=f'Threshold = {certainty_threshold}')\n",
    "    axes[1].set_xlabel('Prediction Certainty')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of Prediction Certainties')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return wknn, scaler\n",
    "\n",
    "\n",
    "# Run Task 4.02\n",
    "wknn_model, wknn_scaler = run_weighted_knn(outlier_checked_path, k=5, certainty_threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The certainty histogram shows how confident the model is across all test predictions. Predictions below the threshold are labelled \"Uncertain\" — this is preferable to misidentification in a real-world recycling context, where misidentified polymers could contaminate the recycling stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Task 4.03: Extension — Grid Search and SVM\n\n**Objective:** Try two additional approaches to improve classification:\n\n**1. Grid search for optimal `k`:**\n- So far we've used k=5 (look at 5 neighbours), but is 5 the best choice? Maybe k=3 or k=11 works better.\n- **Grid search** systematically tries every combination of `k` from 1 to 20 and `weights` (uniform vs distance), evaluates each using **5-fold cross-validation** (splits the training data 5 ways and averages performance), and picks the best combination.\n- `scoring='f1_macro'` means we optimise for the macro-averaged F1 score — this gives equal importance to all polymer classes regardless of how many spectra they have.\n\n**2. Support Vector Machine (SVM) as an alternative classifier:**\n- SVM works differently from kNN: instead of finding nearest neighbours, it finds the **optimal boundary** (hyperplane) that separates the polymer classes in feature space.\n- The **RBF (Radial Basis Function) kernel** allows SVM to draw curved (non-linear) boundaries, which can better separate classes that aren't linearly separable in the 128-dimensional wavelength space.\n- Comparing SVM to kNN tells us whether a fundamentally different algorithm performs better on this data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def grid_search_knn(filepath, test_size=0.5):\n",
    "    \"\"\"Find optimal k for kNN using grid search with cross-validation.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Grid search over k values\n",
    "    param_grid = {'n_neighbors': list(range(1, 21)), 'weights': ['uniform', 'distance']}\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='f1_macro')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f'Best parameters: {grid.best_params_}')\n",
    "    print(f'Best cross-validation F1 score: {grid.best_score_:.3f}')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = grid.predict(X_test_scaled)\n",
    "    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    print(f'\\nTest set Classification Report (optimised kNN):')\n",
    "    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def run_svm_classification(filepath, test_size=0.5):\n",
    "    \"\"\"Train and evaluate an SVM classifier for comparison.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    \n",
    "    labels = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    print('SVM Classification Report:')\n",
    "    print(classification_report(y_test, y_pred, labels=labels, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_title('Confusion Matrix (SVM, RBF kernel)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return svm\n",
    "\n",
    "\n",
    "# Run grid search\n",
    "print('=== Grid Search for Optimal kNN Parameters ===')\n",
    "best_knn = grid_search_knn(outlier_checked_path)\n",
    "\n",
    "print('\\n=== SVM Classifier ===')\n",
    "svm_model = run_svm_classification(outlier_checked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the optimised kNN, weighted kNN, and SVM results helps determine which model is best suited for this spectral classification task. SVM with an RBF kernel can capture non-linear decision boundaries, which may improve classification of spectrally similar polymers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Model Performance: Before vs After Outlier Removal\n",
    "\n",
    "**Objective:** Quantify the impact of the outlier removal step (Task 3.02) on classification accuracy. Three models -- standard kNN (k=5), distance-weighted kNN (k=5), and SVM (RBF kernel) -- are trained and evaluated on both the pre-outlier dataset (`2.07_checked-data.csv`) and the post-outlier dataset (`3.02_outlier-checked-data.csv`). Overall accuracy and macro-averaged F1-score are compared side-by-side to determine whether removing outlier spectra improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_models_on_dataset(filepath, dataset_label):\n",
    "    \"\"\"Train kNN, weighted kNN, and SVM on a dataset; return accuracy, macro F1, and per-class F1.\"\"\"\n",
    "    X, y, _ = prepare_spectral_data(filepath)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    class_labels = sorted(np.unique(y))\n",
    "    results = []\n",
    "\n",
    "    models = [\n",
    "        ('kNN (k=5)', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('Weighted kNN (k=5)', KNeighborsClassifier(n_neighbors=5, weights='distance')),\n",
    "        ('SVM (RBF)', SVC(kernel='rbf', random_state=42)),\n",
    "    ]\n",
    "\n",
    "    for model_name, model in models:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        row = {\n",
    "            'Dataset': dataset_label,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Macro F1': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        }\n",
    "\n",
    "        # Per-class F1 scores\n",
    "        per_class = f1_score(y_test, y_pred, labels=class_labels, average=None, zero_division=0)\n",
    "        for cls, score in zip(class_labels, per_class):\n",
    "            row[f'F1 {cls}'] = score\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return results, class_labels\n",
    "\n",
    "\n",
    "# Evaluate on both datasets\n",
    "before_results, class_labels_before = evaluate_models_on_dataset(checked_path, 'Before Outlier Removal')\n",
    "after_results, class_labels_after = evaluate_models_on_dataset(outlier_checked_path, 'After Outlier Removal')\n",
    "\n",
    "# Combine into a comparison DataFrame\n",
    "comparison_df = pd.DataFrame(before_results + after_results)\n",
    "\n",
    "# Identify per-class F1 columns\n",
    "all_class_labels = sorted(set(class_labels_before) | set(class_labels_after))\n",
    "per_class_cols = [f'F1 {cls}' for cls in all_class_labels]\n",
    "\n",
    "# Format numeric columns\n",
    "for col in ['Accuracy', 'Macro F1'] + per_class_cols:\n",
    "    if col in comparison_df.columns:\n",
    "        comparison_df[col] = comparison_df[col].map('{:.3f}'.format)\n",
    "\n",
    "# --- Summary table (Accuracy + Macro F1) ---\n",
    "summary_cols = ['Dataset', 'Model', 'Accuracy', 'Macro F1']\n",
    "pivot_acc = comparison_df.pivot(index='Model', columns='Dataset', values='Accuracy')\n",
    "pivot_f1 = comparison_df.pivot(index='Model', columns='Dataset', values='Macro F1')\n",
    "\n",
    "print('=== Accuracy Comparison ===')\n",
    "print(pivot_acc[['Before Outlier Removal', 'After Outlier Removal']].to_string())\n",
    "print()\n",
    "print('=== Macro-Averaged F1 Comparison ===')\n",
    "print(pivot_f1[['Before Outlier Removal', 'After Outlier Removal']].to_string())\n",
    "print()\n",
    "\n",
    "# --- Per-class F1 table ---\n",
    "print('=== Per-Class F1 Scores ===')\n",
    "display_cols = ['Dataset', 'Model'] + per_class_cols\n",
    "print(comparison_df[display_cols].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# --- Full table ---\n",
    "print('=== Full Comparison Table ===')\n",
    "print(comparison_df.to_string(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "The tables above show how each model's accuracy, macro-averaged F1-score, and per-class F1-scores change after outlier removal. Key observations:\n",
    "\n",
    "- If the **After** scores are higher, outlier removal successfully cleaned noisy or mislabelled spectra that were confusing the classifiers. This confirms the value of the Pearson correlation-based outlier detection in Task 3.02.\n",
    "- If the scores are similar or slightly lower, the removed spectra may not have been harming classification -- or the smaller dataset may have reduced the model's ability to generalise.\n",
    "- The **macro-averaged F1** is particularly informative for imbalanced datasets because it weights all polymer classes equally, preventing high-frequency classes from masking poor performance on rare ones.\n",
    "- The **per-class F1 scores** reveal which specific polymer types benefit most from outlier removal. Polymers that had outlier spectra removed should show the largest improvements, since noisy samples in those classes would have distorted the decision boundaries.\n",
    "- Among the three models, SVM (RBF) typically benefits the most from outlier removal because support vectors near the decision boundary are sensitive to noisy data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Activity 5: Identifying Your Unknown Polymers\n",
    "\n",
    "**Objective:** Use the best-performing trained model to classify the 10 unknown polymer samples. The unknown spectra must first be processed through the same pipeline (transform, check for invalid values) before being fed to the model.\n",
    "\n",
    "*Update the `UNKNOWN_CSV` path below once you have collected the unknown spectra using PlasTell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: update this path with your actual unknown samples export\n",
    "UNKNOWN_CSV = os.path.join(DATA_DIR, 'unknown_samples.csv')  # UPDATE THIS\n",
    "\n",
    "\n",
    "def identify_unknowns(model, scaler, unknown_filepath, reference_filepath):\n",
    "    \"\"\"Process unknown spectra and predict polymer identity.\"\"\"\n",
    "    if not os.path.exists(unknown_filepath):\n",
    "        print(f'Unknown samples file not found: {unknown_filepath}')\n",
    "        print('Please collect unknown spectra and update the UNKNOWN_CSV path.')\n",
    "        return None\n",
    "    \n",
    "    # Load and transform unknown data (same pipeline as training data)\n",
    "    unknown_raw = pd.read_csv(unknown_filepath)\n",
    "    unknown_raw['spectrum'] = unknown_raw['spectrum'].apply(ast.literal_eval)\n",
    "    \n",
    "    X_unknown = np.array(unknown_raw['spectrum'].tolist())\n",
    "    X_unknown_scaled = scaler.transform(X_unknown)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_unknown_scaled)\n",
    "    \n",
    "    # If model supports predict_proba, show certainties\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probas = model.predict_proba(X_unknown_scaled)\n",
    "        certainties = np.max(probas, axis=1)\n",
    "    else:\n",
    "        certainties = ['N/A'] * len(predictions)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Sample': range(1, len(predictions) + 1),\n",
    "        'Predicted Polymer': predictions,\n",
    "        'Certainty': certainties\n",
    "    })\n",
    "    \n",
    "    print('Unknown Sample Predictions:')\n",
    "    print(results.to_string(index=False))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run Task 5 (will print guidance if unknown file doesn't exist yet)\n",
    "unknown_results = identify_unknowns(wknn_model, wknn_scaler, UNKNOWN_CSV, outlier_checked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical assessment:** When evaluating the model's predictions on unknown samples, consider:\n",
    "- Are the certainty values consistently high? Low certainty may indicate the unknown polymer is not well-represented in the training data.\n",
    "- Could the unknown polymer be a type not present in the reference database? kNN will always assign one of the known classes, even if the true answer is none of them.\n",
    "- Physical/chemical factors such as polymer colour, thickness, additives, and surface texture can all influence the NIR spectrum and reduce classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n### PCA Visualisation of Polymer Feature Space\n\n**Principal Component Analysis (PCA)** is a dimensionality reduction technique that projects high-dimensional data (here, 128 wavelength features per spectrum) onto a smaller set of orthogonal axes called **principal components**. Each component captures the maximum remaining variance in the data.\n\nFor NIR spectral data, PCA is useful because:\n- It reveals the **natural clustering** of polymer types in a reduced feature space.\n- It shows whether polymers that are spectrally similar (and therefore harder to classify) overlap in the low-dimensional projection.\n- The **scree plot** indicates how many components are needed to capture most of the variance — if the first 2–3 components explain >90% of variance, the spectral differences between polymers are dominated by a few key absorption features.\n- Overlaying outlier spectra on the PCA projection shows whether removed outliers genuinely fall outside the main clusters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# ── 1. Load cleaned (post-outlier) data and fit PCA ──────────────────────────\nX_clean, y_clean, wl_clean = prepare_spectral_data(outlier_checked_path)\n\nscaler_pca = StandardScaler()\nX_clean_scaled = scaler_pca.fit_transform(X_clean)\n\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X_clean_scaled)\n\nprint(f'Post-outlier dataset: {X_clean.shape[0]} spectra, {X_clean.shape[1]} features')\nprint(f'Explained variance (first 10 PCs): {pca.explained_variance_ratio_.sum():.1%}')\nprint(f'Per-component: {[f\"{v:.1%}\" for v in pca.explained_variance_ratio_]}')\n\n# ── 2. Scree plot ─────────────────────────────────────────────────────────────\nfig, ax = plt.subplots(figsize=(10, 5))\ncomponents = np.arange(1, 11)\nbars = ax.bar(components, pca.explained_variance_ratio_ * 100, color='steelblue',\n              edgecolor='black', alpha=0.8, label='Individual')\ncumulative = np.cumsum(pca.explained_variance_ratio_) * 100\nax.plot(components, cumulative, 'ro-', linewidth=2, label='Cumulative')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance (%)')\nax.set_title('Scree Plot — Explained Variance by Principal Component')\nax.set_xticks(components)\nax.legend(loc='center right')\nax.set_ylim(0, 105)\nfor i, (bar, val) in enumerate(zip(bars, pca.explained_variance_ratio_ * 100)):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n            f'{val:.1f}%', ha='center', va='bottom', fontsize=8)\nplt.tight_layout()\nplt.show()\n\n# ── 3. 2D scatter plot (PC1 vs PC2) ──────────────────────────────────────────\nunique_labels = sorted(np.unique(y_clean))\ncmap = plt.cm.tab10\ncolors = {label: cmap(i / max(len(unique_labels) - 1, 1)) for i, label in enumerate(unique_labels)}\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfor label in unique_labels:\n    mask = y_clean == label\n    ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n               c=[colors[label]], label=label, s=50, alpha=0.7, edgecolors='k', linewidths=0.3)\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax.set_title('PCA — Polymer Spectra Projected onto PC1 vs PC2')\nax.legend(title='Polymer', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\nplt.tight_layout()\nplt.show()\n\n# ── 4. 3D scatter plot (PC1 vs PC2 vs PC3) ───────────────────────────────────\nfig = plt.figure(figsize=(11, 8))\nax3d = fig.add_subplot(111, projection='3d')\nfor label in unique_labels:\n    mask = y_clean == label\n    ax3d.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n                 c=[colors[label]], label=label, s=40, alpha=0.7, edgecolors='k', linewidths=0.3)\nax3d.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax3d.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax3d.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\nax3d.set_title('PCA — 3D Projection (PC1, PC2, PC3)')\nax3d.legend(title='Polymer', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\nplt.tight_layout()\nplt.show()\n\n# ── 5. Overlay plot: pre-outlier data with outliers highlighted ───────────────\nX_pre, y_pre, _ = prepare_spectral_data(checked_path)\nX_post, y_post, _ = prepare_spectral_data(outlier_checked_path)\n\n# Identify outlier columns: present in pre-outlier but absent from post-outlier\ndf_pre = pd.read_csv(checked_path)\ndf_post = pd.read_csv(outlier_checked_path)\npre_cols = set(df_pre.columns[1:])\npost_cols = set(df_post.columns[1:])\noutlier_col_names = pre_cols - post_cols\nprint(f'Outlier spectra identified: {len(outlier_col_names)}')\nif outlier_col_names:\n    print(f'  Columns: {sorted(outlier_col_names)}')\n\n# Scale ALL pre-outlier data using the same scaler fitted on clean data\nX_pre_scaled = scaler_pca.transform(X_pre)\nX_pre_pca = pca.transform(X_pre_scaled)\n\n# Build mask: True for outlier spectra\npre_col_list = list(df_pre.columns[1:])\noutlier_mask = np.array([col in outlier_col_names for col in pre_col_list])\nnormal_mask = ~outlier_mask\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot normal (non-outlier) points coloured by polymer\nfor label in unique_labels:\n    mask = (y_pre == label) & normal_mask\n    if mask.any():\n        ax.scatter(X_pre_pca[mask, 0], X_pre_pca[mask, 1],\n                   c=[colors[label]], label=label, s=50, alpha=0.6,\n                   edgecolors='k', linewidths=0.3)\n\n# Plot outliers with red 'x' markers\nif outlier_mask.any():\n    ax.scatter(X_pre_pca[outlier_mask, 0], X_pre_pca[outlier_mask, 1],\n               c='red', marker='x', s=100, linewidths=2, zorder=5,\n               label=f'Outliers (n={outlier_mask.sum()})')\n\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nax.set_title('PCA — Pre-Outlier Data with Removed Outliers Highlighted')\nax.legend(title='Polymer', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Interpreting the PCA results:**\n\n- **Scree plot:** The first few principal components typically capture the majority of spectral variance. A sharp \"elbow\" in the scree plot indicates that most meaningful variation is concentrated in just 2--3 dimensions, which is expected for NIR data where polymer identity is primarily encoded in a handful of absorption bands.\n\n- **2D scatter (PC1 vs PC2):** Well-separated clusters indicate that the corresponding polymers have distinct spectral signatures and should be easy to classify. Overlapping clusters suggest spectral similarity between those polymer types -- these are the pairs most likely to be confused by kNN or SVM classifiers (consistent with off-diagonal entries in the confusion matrices above).\n\n- **3D scatter (PC1--PC3):** Adding the third component can resolve clusters that overlap in the 2D projection. If two polymer groups overlap in PC1-PC2 but separate along PC3, this indicates that a third spectral feature distinguishes them.\n\n- **Outlier overlay:** Outlier spectra (red crosses) that fall far from their polymer's cluster confirm that the Pearson correlation-based detection in Task 3.02 correctly identified anomalous measurements. Outliers that fall *within* another polymer's cluster may represent mislabelled samples or contaminated measurements. The spatial separation between outliers and their nominal class provides visual justification for their removal from the training data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance\n",
    "\n",
    "The dataset is severely imbalanced — some polymer classes (e.g., PMMA) have many more spectra than others (e.g., ABS with only a handful). Standard kNN treats all training samples equally, so it is biased toward predicting the majority class. This means minority classes may be consistently misclassified even when the model reports high overall accuracy.\n",
    "\n",
    "Three strategies are compared below:\n",
    "1. **SMOTE (Synthetic Minority Oversampling Technique):** Generates synthetic training samples for minority classes by interpolating between existing minority-class neighbours. This balances the training set without discarding data.\n",
    "2. **Random Undersampling:** Reduces the majority class(es) by randomly removing training samples until all classes have equal representation. Simple but discards potentially useful data.\n",
    "3. **Class-Weighted kNN:** Instead of modifying the training data, sample weights inversely proportional to class frequency are used during training so that minority-class samples carry more influence.\n",
    "\n",
    "All resampling is applied **only to the training set** to avoid data leakage into the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --- Load and split the cleaned data ---\n",
    "X_imb, y_imb, _ = prepare_spectral_data(outlier_checked_path)\n",
    "\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.5, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "scaler_imb = StandardScaler()\n",
    "X_train_scaled_imb = scaler_imb.fit_transform(X_train_imb)\n",
    "X_test_scaled_imb = scaler_imb.transform(X_test_imb)\n",
    "\n",
    "print('Training set class distribution (before resampling):')\n",
    "print(pd.Series(y_train_imb).value_counts())\n",
    "print()\n",
    "\n",
    "# Store results for comparison\n",
    "results_list = []\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Baseline: standard kNN (no balancing)\n",
    "# -------------------------------------------------------------------\n",
    "knn_baseline = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_baseline.fit(X_train_scaled_imb, y_train_imb)\n",
    "y_pred_baseline = knn_baseline.predict(X_test_scaled_imb)\n",
    "\n",
    "report_baseline = classification_report(\n",
    "    y_test_imb, y_pred_baseline, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Baseline (no balancing)',\n",
    "    'Macro F1': report_baseline['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_baseline['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_baseline['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Baseline kNN (no balancing) ===')\n",
    "print(classification_report(y_test_imb, y_pred_baseline, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. SMOTE oversampling on training set\n",
    "# -------------------------------------------------------------------\n",
    "# SMOTE needs at least k_neighbors+1 samples per minority class.\n",
    "# Determine a safe k_neighbors value for SMOTE.\n",
    "min_train_count = min(Counter(y_train_imb).values())\n",
    "smote_k = min(5, min_train_count - 1) if min_train_count > 1 else 1\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=smote_k)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled_imb, y_train_imb)\n",
    "\n",
    "print(f'SMOTE resampled training set distribution (k_neighbors={smote_k}):')\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "print()\n",
    "\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = knn_smote.predict(X_test_scaled_imb)\n",
    "\n",
    "report_smote = classification_report(\n",
    "    y_test_imb, y_pred_smote, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'SMOTE oversampling',\n",
    "    'Macro F1': report_smote['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_smote['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_smote['accuracy']\n",
    "})\n",
    "\n",
    "print('=== SMOTE Oversampling ===')\n",
    "print(classification_report(y_test_imb, y_pred_smote, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Random undersampling on training set\n",
    "# -------------------------------------------------------------------\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled_imb, y_train_imb)\n",
    "\n",
    "print('Undersampled training set distribution:')\n",
    "print(pd.Series(y_train_rus).value_counts())\n",
    "print()\n",
    "\n",
    "knn_rus = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_rus.fit(X_train_rus, y_train_rus)\n",
    "y_pred_rus = knn_rus.predict(X_test_scaled_imb)\n",
    "\n",
    "report_rus = classification_report(\n",
    "    y_test_imb, y_pred_rus, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Random undersampling',\n",
    "    'Macro F1': report_rus['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_rus['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_rus['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Random Undersampling ===')\n",
    "print(classification_report(y_test_imb, y_pred_rus, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Class-weighted kNN using sample weights\n",
    "# -------------------------------------------------------------------\n",
    "# sklearn's KNeighborsClassifier does not natively support class_weight,\n",
    "# so we use a distance-weighted kNN and compute inverse-frequency sample\n",
    "# weights via compute_sample_weight('balanced', ...). We wrap the\n",
    "# approach by fitting on the original training set but weighting\n",
    "# neighbours by class frequency during prediction using a custom scorer.\n",
    "#\n",
    "# Practical approach: use compute_sample_weight to get per-sample weights,\n",
    "# then duplicate minority samples proportionally to approximate weighting.\n",
    "\n",
    "sample_weights = compute_sample_weight('balanced', y_train_imb)\n",
    "\n",
    "# Duplicate samples according to their weight (rounded to integers)\n",
    "repeat_counts = np.round(sample_weights).astype(int)\n",
    "repeat_counts = np.maximum(repeat_counts, 1)  # At least 1 copy\n",
    "\n",
    "X_train_weighted = np.repeat(X_train_scaled_imb, repeat_counts, axis=0)\n",
    "y_train_weighted = np.repeat(y_train_imb, repeat_counts, axis=0)\n",
    "\n",
    "print('Class-weighted (duplicated) training set distribution:')\n",
    "print(pd.Series(y_train_weighted).value_counts())\n",
    "print()\n",
    "\n",
    "knn_weighted = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_weighted.fit(X_train_weighted, y_train_weighted)\n",
    "y_pred_weighted = knn_weighted.predict(X_test_scaled_imb)\n",
    "\n",
    "report_weighted = classification_report(\n",
    "    y_test_imb, y_pred_weighted, output_dict=True, zero_division=0\n",
    ")\n",
    "results_list.append({\n",
    "    'Method': 'Class-weighted kNN',\n",
    "    'Macro F1': report_weighted['macro avg']['f1-score'],\n",
    "    'Weighted F1': report_weighted['weighted avg']['f1-score'],\n",
    "    'Accuracy': report_weighted['accuracy']\n",
    "})\n",
    "\n",
    "print('=== Class-Weighted kNN ===')\n",
    "print(classification_report(y_test_imb, y_pred_weighted, zero_division=0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Comparison table\n",
    "# -------------------------------------------------------------------\n",
    "comparison_df = pd.DataFrame(results_list)\n",
    "comparison_df = comparison_df.set_index('Method')\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('CLASS IMBALANCE STRATEGY COMPARISON')\n",
    "print('=' * 60)\n",
    "print(comparison_df.to_string())\n",
    "print('=' * 60)\n",
    "\n",
    "# --- Confusion matrices side-by-side ---\n",
    "labels_imb = sorted(np.unique(np.concatenate([y_test_imb, y_pred_baseline])))\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n",
    "titles = ['Baseline (no balancing)', 'SMOTE', 'Random Undersampling', 'Class-Weighted kNN']\n",
    "predictions = [y_pred_baseline, y_pred_smote, y_pred_rus, y_pred_weighted]\n",
    "\n",
    "for ax, title, y_pred in zip(axes, titles, predictions):\n",
    "    cm = confusion_matrix(y_test_imb, y_pred, labels=labels_imb)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels_imb, yticklabels=labels_imb, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrices: Class Imbalance Strategies', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the class imbalance results:**\n",
    "\n",
    "The comparison table above shows how each balancing strategy affects classification performance:\n",
    "\n",
    "- **Macro F1** treats all classes equally regardless of size, making it the most important metric when minority-class performance matters (e.g., correctly identifying a rare polymer in a recycling stream).\n",
    "- **Weighted F1** gives more importance to larger classes, so it tends to look favourable even when minority classes are poorly classified.\n",
    "- **Accuracy** can be misleading with imbalanced data — a model that always predicts the majority class can still achieve high accuracy.\n",
    "\n",
    "**Trade-offs between approaches:**\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|---|---|---|\n",
    "| SMOTE | Preserves all real data; generates plausible synthetic samples | May create unrealistic spectra if classes overlap in feature space; requires enough minority samples to interpolate |\n",
    "| Random Undersampling | Simple; fast; no synthetic data | Discards majority-class data that may contain useful information; high variance with very small datasets |\n",
    "| Class-Weighted kNN | No data modification; accounts for imbalance during model fitting | Approximate (uses sample duplication); may over-represent noisy minority samples |\n",
    "\n",
    "In practice, the best strategy depends on the degree of imbalance and the number of available samples. When minority classes have very few spectra (e.g., fewer than 5), SMOTE may struggle because it needs neighbours to interpolate between, while undersampling would reduce the training set to an impractically small size. In such cases, class-weighted approaches or collecting additional data for under-represented polymers may be the most practical solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}